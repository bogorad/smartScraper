{"id":"smartScraper-0vk","title":"Missing Input Validation on Dashboard Parameters","description":"Dashboard routes accept query params without validation.\n\n## Problem\nQuery parameters (page, limit, sort) at sites.tsx:20-23 are used directly without Zod validation. parseInt on invalid input returns NaN, potentially causing unexpected behavior or errors downstream.\n\n## Location\n- src/routes/dashboard/sites.tsx:20-23\n\n## Risk\nMalformed query parameters could cause application errors or unexpected behavior. Low security risk but poor code quality.\n\n## Action Plan\n1. Create Zod schema for dashboard query parameters:\n   ```typescript\n   const querySchema = z.object({\n     q: z.string().optional().default(''),\n     sort: z.enum(['domain', 'failures', 'last']).optional().default('domain'),\n     limit: z.union([z.literal('all'), z.coerce.number().int().min(1).max(100)]).optional().default(10),\n     page: z.coerce.number().int().min(1).optional().default(1)\n   });\n   ```\n2. Apply zValidator middleware or manual parse at route handler start\n3. Use parsed/validated values instead of raw query strings\n4. Return 400 Bad Request for invalid parameters with helpful message\n5. Add tests for valid params, invalid params, and edge cases\n\n## Acceptance Criteria\n- All query parameters are validated before use\n- Invalid parameters return 400 with descriptive error\n- Default values are applied for missing optional params\n- parseInt is never called on unvalidated strings\n- Tests cover validation scenarios","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:05.189012091+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:47:09.70847125+01:00","closed_at":"2026-01-31T13:47:09.70847125+01:00","close_reason":"Fixed: Added Zod validation schema for dashboard query parameters (q, sort, limit, page) with proper types and constraints.","labels":["medium","security"]}
{"id":"smartScraper-1q4","title":"No Rate Limiting on API Endpoints","description":"API endpoints have no rate limiting, allowing unlimited requests from authenticated clients.\n\n## Problem\nEnables:\n1. DoS via resource exhaustion (browser instances, memory, CPU)\n2. Cost abuse on external services (2Captcha charges per solve, OpenRouter charges per token)\n3. Target website abuse leading to IP bans and potential legal liability\n\n## Location\n- src/routes/api/scrape.ts\n- src/routes/dashboard/*.tsx\n\n## Risk\nResource exhaustion, runaway costs, legal issues from aggressive scraping. Any authenticated user can trigger unlimited scrapes.\n\n## Action Plan\n1. Create src/middleware/rate-limit.ts with rateLimitMiddleware factory\n2. Use in-memory store with sliding window: Map\u003cstring, {count, resetTime}\u003e\n3. Identify clients by Authorization header or x-forwarded-for IP\n4. Configure different limits per endpoint:\n   - /api/scrape: 10 requests/minute (scraping is expensive)\n   - /dashboard/*: 60 requests/minute (UI interactions)\n5. Return 429 Too Many Requests with Retry-After header\n6. Add rate limit headers to responses (X-RateLimit-Remaining, X-RateLimit-Reset)\n7. Apply middleware to scrapeRouter and dashboardRouter\n8. Add configuration options via config.ts for limit customization\n9. Write tests for rate limit enforcement and reset behavior\n\n## Acceptance Criteria\n- Requests exceeding limit receive 429 response\n- Rate limit headers present on all responses\n- Limits are configurable via environment variables\n- Legitimate usage patterns are not impacted\n- Tests verify limit enforcement and window reset","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:41.803170249+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:41:52.701476912+01:00","closed_at":"2026-01-31T13:41:52.701476912+01:00","close_reason":"Fixed: Implemented rate limiting middleware with 10 req/min for API and 60 req/min for dashboard. Includes rate limit headers and 429 responses.","labels":["critical","security"]}
{"id":"smartScraper-2gg","title":"2Captcha Polling Continues After Fatal Errors","description":"solveDataDome does not detect all fatal 2Captcha API errors.\n\n## Problem\nError detection at lines 136-138 checks for status=error OR non-zero errorId, but some fatal conditions may have status=processing with an errorCode field. This causes wasted polling iterations and delayed failure responses.\n\n## Location\n- src/adapters/twocaptcha.ts:116-139\n\n## Risk\nWasted API calls (cost), delayed user feedback, potential timeout before error is properly reported.\n\n## Action Plan\n1. Research 2Captcha API documentation for all possible error response formats\n2. Identify fatal error codes that should terminate polling immediately:\n   - ERROR_CAPTCHA_UNSOLVABLE\n   - ERROR_WRONG_CAPTCHA_ID\n   - ERROR_BAD_TOKEN_OR_PAGEURL\n   - etc.\n3. Update polling loop condition to check:\n   - status === 'error'\n   - errorId !== 0\n   - presence of errorCode field (regardless of status)\n4. Extract error checking into helper function: isFatalError(response): boolean\n5. Add specific error messages for known fatal codes\n6. Add test cases with mocked 2Captcha responses for each error type\n\n## Acceptance Criteria\n- All fatal 2Captcha errors terminate polling immediately\n- Specific error messages returned for known error codes\n- No unnecessary polling iterations after fatal error\n- Tests cover all documented error response formats","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:14.749452616+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:49:31.121989655+01:00","closed_at":"2026-01-31T13:49:31.121989655+01:00","close_reason":"Fixed: Enhanced error detection to check for errorCode field regardless of status. Added mapping for known fatal error codes to provide better error messages and terminate polling immediately.","labels":["medium","quality"]}
{"id":"smartScraper-5kf","title":"No CSRF Protection on Dashboard Forms","description":"Dashboard POST endpoints lack CSRF protection tokens.\n\n## Problem\nPOST endpoints like /dashboard/sites/:domain do not implement CSRF tokens. HTMX requests from authenticated sessions are vulnerable to cross-site request forgery attacks.\n\n## Location\n- src/routes/dashboard/sites.tsx:269-360\n\n## Risk\nAttackers could trick authenticated users into performing unwanted actions (deleting sites, modifying configurations) via malicious links or embedded forms on third-party sites.\n\n## Action Plan\n1. Create src/middleware/csrf.ts with csrfMiddleware\n2. On GET requests: generate crypto.randomUUID() token, store in cookie (httpOnly: false for JS access)\n3. Set token in Hono context: c.set('csrfToken', token)\n4. On POST/PUT/DELETE: validate X-CSRF-Token header matches cookie\n5. Return 403 Forbidden if validation fails\n6. Update layout.tsx to include CSRF token in a meta tag\n7. Configure HTMX to send X-CSRF-Token header on all requests via hx-headers\n8. Apply csrfMiddleware to dashboardRouter\n9. Write tests for token generation, validation success, and validation failure\n\n## Acceptance Criteria\n- All dashboard POST requests require valid CSRF token\n- Missing or invalid token returns 403 response\n- HTMX requests automatically include token via configured headers\n- Token regenerates on each page load\n- Tests cover happy path and attack scenarios","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:49.772971002+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:43:38.021221655+01:00","closed_at":"2026-01-31T13:43:38.021221655+01:00","close_reason":"Fixed: Implemented CSRF middleware with token generation on GET and validation on POST/PUT/DELETE. HTMX configured to send X-CSRF-Token header automatically.","labels":["high","security"]}
{"id":"smartScraper-6ax","title":"Missing Error Handling in Async Operations","description":"Empty catch blocks swallow errors silently.\n\n## Problem\nSeveral async operations use empty catch blocks, hiding critical errors from logs and monitoring. This makes debugging difficult and can mask underlying issues.\n\n## Location\n- src/adapters/puppeteer-browser.ts:33-35 (closePage cleanup)\n- src/core/engine.ts:296-300 (finally block cleanup)\n\n## Risk\nErrors go undetected, making debugging difficult. Potential resource leaks if cleanup fails silently. Production issues may be impossible to diagnose.\n\n## Action Plan\n1. Identify all empty catch blocks in codebase: `rg 'catch.*\\{\\s*\\}' src/`\n2. For each empty catch, determine appropriate handling:\n   - Expected failures (e.g., closing already-closed page): log at DEBUG level\n   - Unexpected failures: log at WARN level with error details\n3. Update puppeteer-browser.ts:33-35:\n   ```typescript\n   try { await session.page.close(); } \n   catch (e) { logger.debug('Page close failed (may already be closed)', { error: String(e) }); }\n   ```\n4. Update engine.ts:296-300 similarly\n5. Review other catch blocks for completeness\n6. Add eslint rule to prevent empty catch blocks: no-empty\n\n## Acceptance Criteria\n- No empty catch blocks remain in codebase\n- All caught errors are logged with appropriate level\n- Expected failures logged at DEBUG (not noisy)\n- Unexpected failures logged at WARN (visible)\n- ESLint configured to prevent future empty catches","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:17.187347472+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:50:18.474446152+01:00","closed_at":"2026-01-31T13:50:18.474446152+01:00","close_reason":"Fixed: Added proper error logging to empty catch blocks in puppeteer-browser.ts and engine.ts. Errors are logged at DEBUG level since they represent expected cleanup failures.","labels":["medium","quality"]}
{"id":"smartScraper-92n","title":"Information Disclosure in Error Messages","description":"Error responses expose internal implementation details.\n\n## Problem\nSeveral error responses leak stack traces, file paths, and internal state that could aid attackers in reconnaissance. Examples include raw error.message from exceptions and detailed 2Captcha API responses.\n\n## Location\n- src/routes/api/scrape.ts\n- src/adapters/twocaptcha.ts\n- src/core/engine.ts\n\n## Risk\nInformation leakage aids attackers in understanding system architecture, identifying vulnerable components, and crafting targeted attacks.\n\n## Action Plan\n1. Create src/utils/error-sanitizer.ts with sanitizeErrorForClient(error): string function\n2. Define allowlist of safe error messages to expose (e.g., 'Invalid URL', 'Rate limit exceeded')\n3. Map internal errors to generic client-facing messages\n4. Update scrape.ts error handler to use sanitized messages in JSON response\n5. Keep detailed error logging internal via logger.error()\n6. Review twocaptcha.ts responses - strip API-specific details\n7. Review engine.ts - ensure stack traces never reach client\n8. Add error response tests verifying no internal details leak\n\n## Acceptance Criteria\n- Client-facing error responses contain only generic, safe messages\n- Stack traces never appear in API responses\n- Internal file paths never appear in API responses\n- Detailed errors are logged server-side for debugging\n- Tests verify error sanitization","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:59.321813794+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:46:09.350958192+01:00","closed_at":"2026-01-31T13:46:09.350958192+01:00","close_reason":"Fixed: Created error-sanitizer.ts with sanitizeErrorForClient() function. Updated scrape endpoint to catch errors and return sanitized messages to clients while logging full details internally.","labels":["medium","security"]}
{"id":"smartScraper-ahl","title":"Potential ReDoS in DOM Simplification","description":"simplifyDom function uses regex patterns vulnerable to catastrophic backtracking.\n\n## Problem\nRegex patterns like `\u003ctag[^\u003e]*\u003e.*?\u003c/tag\u003e` with gis flags on untrusted HTML can cause exponential backtracking with crafted nested structures. The .*? quantifier combined with alternation in complex HTML can trigger ReDoS.\n\n## Location\n- src/utils/dom.ts:8-38\n\n## Risk\nCPU exhaustion via crafted HTML input. Severity is Medium because input comes from scraped pages (not direct user input), but a malicious target site could exploit this.\n\n## Action Plan\n1. Add MAX_HTML_SIZE constant (1MB) at top of file\n2. Truncate input at start of simplifyDom: `if (html.length \u003e MAX_HTML_SIZE) html = html.slice(0, MAX_HTML_SIZE)`\n3. Consider replacing regex approach with DOM-based parsing:\n   - Use linkedom or happy-dom to parse HTML\n   - Use querySelectorAll to find and remove unwanted tags\n   - Use DOM API for class-based removal\n4. If keeping regex, add execution timeout wrapper\n5. Benchmark both approaches for performance comparison\n6. Add test cases with large HTML inputs and nested structures\n\n## Acceptance Criteria\n- HTML inputs over 1MB are truncated before processing\n- Processing completes in bounded time regardless of input structure\n- Existing functionality preserved for normal inputs\n- Performance benchmarks show acceptable overhead\n- No regression in LLM prompt generation quality","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:53.417945548+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:44:23.414944267+01:00","closed_at":"2026-01-31T13:44:23.414944267+01:00","close_reason":"Fixed: Added MAX_HTML_SIZE constant (1MB) to truncate oversized inputs before regex processing, preventing ReDoS attacks via crafted HTML.","labels":["medium","security"]}
{"id":"smartScraper-bh9","title":"console.log Used Instead of Logger","description":"twocaptcha.ts uses console.log directly instead of centralized logger.\n\n## Problem\nLines 102, 109, and 126 use console.log for debug output instead of the project's logger utility. This violates src/AGENTS.md convention and bypasses log level filtering, structured logging, and any log aggregation setup.\n\n## Location\n- src/adapters/twocaptcha.ts:102, 109, 126\n\n## Risk\nLow severity but poor code hygiene. Debug output may appear in production. Inconsistent log format makes parsing difficult.\n\n## Action Plan\n1. Import logger at top of file: `import { logger } from '../utils/logger.js';`\n2. Replace line 102 console.log with logger.debug\n3. Replace line 109 console.log with logger.debug\n4. Replace line 126 console.log with logger.debug\n5. Use structured logging format: `logger.debug('message', { data }, 'CAPTCHA')`\n6. Ensure sensitive data (API keys) remain redacted\n7. Run grep to verify no console.log remains: `rg 'console\\.log' src/`\n\n## Acceptance Criteria\n- No console.log calls in twocaptcha.ts\n- All logging uses project logger utility\n- Log level is DEBUG (not INFO) for verbose API interaction logs\n- API keys remain redacted in logs\n- Grep confirms no console.log in src/","status":"closed","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:20.349170684+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:51:31.824106838+01:00","closed_at":"2026-01-31T13:51:31.824106838+01:00","close_reason":"Fixed: Replaced all console.log calls with logger.debug() in twocaptcha.ts. API keys remain properly redacted in log output.","labels":["low","quality"]}
{"id":"smartScraper-cnv","title":"Potential Memory Leak in SSE Connections","description":"SSE implementation lacks connection limits and stale cleanup.\n\n## Problem\nThe clients Set at index.tsx:44 has no maximum connection limit. No timeout mechanism exists for stale connections that fail to disconnect properly, potentially leading to memory exhaustion.\n\n## Location\n- src/routes/dashboard/index.tsx:39-163\n\n## Risk\nMemory leak if connections accumulate. Potential DoS vector if attacker opens many SSE connections.\n\n## Action Plan\n1. Add MAX_SSE_CLIENTS constant (e.g., 100)\n2. Check clients.size before adding new client in /events handler\n3. Return 503 Service Unavailable if limit exceeded\n4. Add connection timestamp to Client interface\n5. Implement periodic cleanup (every 60s) to remove connections older than 10 minutes\n6. Add heartbeat response tracking - remove clients that fail to receive heartbeat\n7. Log connection/disconnection events for monitoring\n8. Add metrics endpoint or logging for current client count\n\n## Acceptance Criteria\n- New connections rejected with 503 when limit reached\n- Stale connections cleaned up automatically\n- Memory usage bounded regardless of connection patterns\n- Monitoring visibility into connection count\n- Tests verify limit enforcement and cleanup","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:08.382732445+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:47:57.78626026+01:00","closed_at":"2026-01-31T13:47:57.78626026+01:00","close_reason":"Fixed: Added MAX_SSE_CLIENTS limit (100), connection timeout (10 min), and periodic cleanup of stale connections to prevent memory leaks.","labels":["medium","quality"]}
{"id":"smartScraper-g2v","title":"Insufficient Logging of Security Events","description":"Security events logged at inappropriate levels.\n\n## Problem\nFailed authentication attempts and configuration changes are logged at INFO/DEBUG level in auth.ts. Production log configurations often filter these levels, making security incident investigation difficult.\n\n## Location\n- src/middleware/auth.ts\n\n## Risk\nSecurity incidents may go unnoticed. Audit trail incomplete for compliance requirements. Difficult to detect brute-force attacks or unauthorized access attempts.\n\n## Action Plan\n1. Review all logger calls in auth.ts and identify security-relevant events\n2. Create security event categories:\n   - AUTH_FAILURE: Failed login attempts (WARN level)\n   - AUTH_SUCCESS: Successful logins (INFO level, but always captured)\n   - SESSION_CREATED: New session establishment (INFO level)\n   - SESSION_INVALID: Invalid session presented (WARN level)\n3. Update log levels:\n   - logger.warn for all failure cases (currently some use logger.info)\n   - Include relevant context: IP address, user agent, timestamp\n4. Consider structured logging format for security events\n5. Document security logging in ADR or CONFIGURATION.md\n6. Verify production log config captures WARN and above\n\n## Acceptance Criteria\n- All authentication failures logged at WARN or higher\n- Security events include sufficient context for investigation\n- Production log configuration captures all security events\n- Log format suitable for SIEM ingestion if needed","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:11.072524028+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:48:47.7250999+01:00","closed_at":"2026-01-31T13:48:47.7250999+01:00","close_reason":"Fixed: Enhanced security event logging in auth.ts with WARN level for authentication failures including IP address and user agent context for audit trail.","labels":["medium","security"]}
{"id":"smartScraper-goi","title":"Critical: Hardcoded timeout in browser reload after CAPTCHA solving","description":"## Problem\n\nKimi Audit Issue #2 (Critical) was NOT addressed in the security fixes.\n\nThe browser reload operation after CAPTCHA solving uses a hardcoded timeout value (Puppeteer's default of 30,000 ms) instead of respecting the user-provided `timeoutMs` parameter.\n\nWhen a CAPTCHA is detected and solved, and cookies are updated, the engine calls `browser.reload(pageId)` without passing any timeout parameter. This causes operations to fail with \"Navigation timeout of 30000 ms exceeded\" even when users explicitly provide custom timeout values.\n\n## Locations\n\n- `src/core/engine.ts` line 157 - calls `reload(pageId)` without timeout\n- `src/ports/browser.ts` line 20 - interface lacks timeout option\n- `src/adapters/puppeteer-browser.ts` lines 278-282 - implementation uses Puppeteer default\n\n## Risk\n\nCAPTCHA-protected sites become inaccessible regardless of user timeout configuration, breaking core functionality.\n\n## Action Plan\n\n1. Update `BrowserPort` interface to accept optional timeout/waitUntil in `reload()`\n2. Update `PuppeteerBrowserAdapter.reload()` to use `options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Update `CoreScraperEngine` to pass `timeoutMs` from scrape options to reload call\n4. Add test coverage for timeout propagation\n\n## Fix Reference (from Kimi audit)\n\n\\`\\`\\`typescript\n// Step 1: Update BrowserPort interface\nreload(pageId: string, options?: { timeout?: number; waitUntil?: 'load' | 'domcontentloaded' | 'networkidle0' | 'networkidle2' }): Promise\u003cvoid\u003e;\n\n// Step 2: Update PuppeteerBrowserAdapter\nasync reload(pageId: string, options?: { timeout?: number; waitUntil?: string }): Promise\u003cvoid\u003e {\n  const session = this.sessions.get(pageId);\n  if (!session) return;\n  await session.page.reload({\n    waitUntil: options?.waitUntil || 'networkidle2',\n    timeout: options?.timeout || DEFAULTS.TIMEOUT_MS\n  });\n}\n\n// Step 3: Update CoreScraperEngine call\nif (solveResult.updatedCookie) {\n  await this.browserPort.setCookies(pageId, solveResult.updatedCookie);\n  await this.browserPort.reload(pageId, {\n    timeout: options?.timeoutMs || DEFAULTS.TIMEOUT_MS,\n    waitUntil: 'networkidle2'\n  });\n}\n\\`\\`\\`","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:55:39.843504235+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:56:38.31367606+01:00","closed_at":"2026-01-31T13:56:38.31367606+01:00","close_reason":"Fixed: reload() now uses DEFAULTS.TIMEOUT_MS (120s) as fallback instead of Puppeteer default (30s) when timeoutMs is undefined. The fix was partially in place; this commit ensures consistent timeout behavior."}
{"id":"smartScraper-nw5","title":"XPath Injection Vulnerability","description":"evaluateXPath function passes user-controlled XPath expressions directly to page.evaluate() without validation.\n\n## Problem\nNo validation or sanitization of XPath input allows:\n1. CPU exhaustion via computationally expensive queries (e.g., deeply nested predicates)\n2. Memory exhaustion via queries returning massive result sets\n3. Potential information disclosure via reading unexpected document nodes\n\n## Location\n- src/adapters/puppeteer-browser.ts:152-188\n\n## Risk\nResource exhaustion attacks from authenticated users. Requires valid API token, so severity is High rather than Critical.\n\n## Action Plan\n1. Define allowed XPath character pattern: `/^[\\w\\-\\/\\[\\]@=\"'\\s\\.\\(\\)\\|\\*\\:]+$/`\n2. Set maximum XPath length constant: `MAX_XPATH_LENGTH = 500`\n3. Create validateXPath(xpath: string): boolean function\n4. Add validation call at start of evaluateXPath method\n5. Return null and log warning for invalid XPaths\n6. Add timeout protection inside page.evaluate to limit execution time\n7. Write test cases for valid XPaths, malformed XPaths, and oversized XPaths\n\n## Acceptance Criteria\n- Invalid XPaths are rejected before reaching Puppeteer\n- Oversized XPaths (\u003e500 chars) are rejected\n- Valid XPaths continue to work correctly\n- Rejection is logged with truncated XPath for debugging\n- All existing tests pass","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:38.726979396+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:39:13.136582379+01:00","closed_at":"2026-01-31T13:39:13.136582379+01:00","close_reason":"Fixed: Added XPath validation with max length (500 chars) and allowed character pattern to prevent injection attacks.","labels":["high","security"]}
{"id":"smartScraper-ubh","title":"Inconsistent Default Timeout Values","description":"PuppeteerBrowserAdapter uses inconsistent timeout defaults across operations.\n\n## Problem\n- Browser launch uses DEFAULTS.TIMEOUT_MS (120000ms) at line 77\n- Page navigation uses hardcoded 45000ms at line 105\nThis violates DRY principle and causes non-deterministic timeout behavior. Users setting custom timeouts may be confused when navigation uses different defaults than other operations.\n\n## Location\n- src/adapters/puppeteer-browser.ts:77, 105\n\n## Risk\nFunctional bugs where navigation times out unexpectedly, or operations take longer than expected. Violates principle of least surprise.\n\n## Action Plan\n1. Locate all hardcoded timeout values in puppeteer-browser.ts\n2. Replace line 105 timeout: `timeout: options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Search for other hardcoded timeouts in the file (reload, waitForSelector, etc.)\n4. Ensure all timeout parameters consistently use DEFAULTS.TIMEOUT_MS as fallback\n5. Consider adding DEFAULTS.NAVIGATION_TIMEOUT if navigation needs different default\n6. Update any related documentation in ADR-001\n7. Add test to verify timeout propagation from options to Puppeteer calls\n\n## Acceptance Criteria\n- All timeout parameters use DEFAULTS.TIMEOUT_MS consistently\n- Custom timeout options are respected throughout the call chain\n- No hardcoded magic numbers for timeouts remain\n- Tests verify timeout behavior","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:56.668782566+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:44:57.752733199+01:00","closed_at":"2026-01-31T13:44:57.752733199+01:00","close_reason":"Fixed: Replaced hardcoded 45000ms timeout with DEFAULTS.TIMEOUT_MS constant for consistent timeout behavior across all operations.","labels":["high","quality"]}
{"id":"smartScraper-xn4","title":"Insecure Session Cookie Configuration","description":"Session cookie in auth.ts:67-68 has secure hardcoded to false, violating ADR-014 adaptive security strategy.\n\n## Problem\nLine 68 forces `const isSecure = false;` with comment 'Force secure: false for now'. This exposes session tokens to interception over HTTP, enabling session hijacking attacks on any non-localhost deployment.\n\n## Location\n- src/middleware/auth.ts:64-78\n\n## Risk\nSession hijacking via network interception on non-HTTPS connections. Critical for any production or LAN deployment.\n\n## Action Plan\n1. Read ADR-014 to understand the intended adaptive security behavior\n2. In createSession(), extract hostname from request: `c.req.header('host')?.split(':')[0]`\n3. Define localhost patterns: `['localhost', '127.0.0.1', '0.0.0.0']`\n4. Implement adaptive logic: `const isSecure = getNodeEnv() === 'production' \u0026\u0026 !isLocalhost`\n5. Add debug logging for security decisions\n6. Write test cases for both localhost and production scenarios\n7. Verify cookie behavior in browser DevTools\n\n## Acceptance Criteria\n- Cookies use secure=true in production on non-localhost hosts\n- Cookies use secure=false on localhost for dev convenience\n- Existing tests pass\n- Manual verification in browser confirms correct cookie flags","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:34.892304795+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:38:36.109465233+01:00","closed_at":"2026-01-31T13:38:36.109465233+01:00","close_reason":"Fixed: Implemented adaptive security per ADR-014. secure=true only in production on non-localhost hosts.","labels":["critical","security"]}
