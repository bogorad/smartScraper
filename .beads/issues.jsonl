{"id":"smartScraper-0rb","title":"Puppeteer 24.34.0 crash due to improper extension loading await","description":"## Bug Report: Puppeteer Unhandled Rejection Crash\n\n**Summary**\nSmartScraper crashes with an `Unhandled Rejection: TargetCloseError` when loading extensions. This is caused by a bug in `puppeteer-core@24.34.0` where extension installation promises are not properly awaited during browser launch.\n\n**Root Cause**\nIn `node_modules/puppeteer-core/lib/esm/puppeteer/node/BrowserLauncher.js` (and source `BrowserLauncher.ts`), the code uses:\n\n```javascript\nawait Promise.all([\n    enableExtensions.map(path =\u003e {\n        return browser.installExtension(path);\n    }),\n]);\n```\n\nThis passes `[[Promise, Promise, ...]]` to `Promise.all`, which resolves immediately without waiting for the inner promises. The extensions attempt to install in the background. When the browser is closed (e.g. after a test/scrape finishes), these pending promises reject with \"Target closed\", causing an unhandled rejection that crashes the process.\n\n**Impact**\n- Application crashes randomly after scrapes when extensions are enabled\n- Test suite fails because Hono server dies\n- Race condition depends on how fast the scrape finishes vs extension loading\n\n**Reproduction**\n1. Enable extensions (default behavior)\n2. Run a quick scrape that closes the browser immediately\n3. Observe process crash with `TargetCloseError: Protocol error (Extensions.loadUnpacked): Target closed`\n\n**Suggested Fixes**\n1. Downgrade Puppeteer to a stable version\n2. Patch the package locally (remove the wrapping array in `Promise.all`)\n3. Fork/PR upstream fix to Puppeteer","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T13:34:57.675096837+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.626329348+01:00","close_reason":"Fixed: Restored Chrome extension flags --enable-extensions and --disable-extensions-except in buildLaunchArgs()"}
{"id":"smartScraper-0vk","title":"Missing Input Validation on Dashboard Parameters","description":"Dashboard routes accept query params without validation.\n\n## Problem\nQuery parameters (page, limit, sort) at sites.tsx:20-23 are used directly without Zod validation. parseInt on invalid input returns NaN, potentially causing unexpected behavior or errors downstream.\n\n## Location\n- src/routes/dashboard/sites.tsx:20-23\n\n## Risk\nMalformed query parameters could cause application errors or unexpected behavior. Low security risk but poor code quality.\n\n## Action Plan\n1. Create Zod schema for dashboard query parameters:\n   ```typescript\n   const querySchema = z.object({\n     q: z.string().optional().default(''),\n     sort: z.enum(['domain', 'failures', 'last']).optional().default('domain'),\n     limit: z.union([z.literal('all'), z.coerce.number().int().min(1).max(100)]).optional().default(10),\n     page: z.coerce.number().int().min(1).optional().default(1)\n   });\n   ```\n2. Apply zValidator middleware or manual parse at route handler start\n3. Use parsed/validated values instead of raw query strings\n4. Return 400 Bad Request for invalid parameters with helpful message\n5. Add tests for valid params, invalid params, and edge cases\n\n## Acceptance Criteria\n- All query parameters are validated before use\n- Invalid parameters return 400 with descriptive error\n- Default values are applied for missing optional params\n- parseInt is never called on unvalidated strings\n- Tests cover validation scenarios","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:05.189012091+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.628506973+01:00","close_reason":"Fixed: Added Zod validation schema for dashboard query parameters (q, sort, limit, page) with proper types and constraints.","labels":["medium","security"]}
{"id":"smartScraper-1q4","title":"No Rate Limiting on API Endpoints","description":"API endpoints have no rate limiting, allowing unlimited requests from authenticated clients.\n\n## Problem\nEnables:\n1. DoS via resource exhaustion (browser instances, memory, CPU)\n2. Cost abuse on external services (2Captcha charges per solve, OpenRouter charges per token)\n3. Target website abuse leading to IP bans and potential legal liability\n\n## Location\n- src/routes/api/scrape.ts\n- src/routes/dashboard/*.tsx\n\n## Risk\nResource exhaustion, runaway costs, legal issues from aggressive scraping. Any authenticated user can trigger unlimited scrapes.\n\n## Action Plan\n1. Create src/middleware/rate-limit.ts with rateLimitMiddleware factory\n2. Use in-memory store with sliding window: Map\u003cstring, {count, resetTime}\u003e\n3. Identify clients by Authorization header or x-forwarded-for IP\n4. Configure different limits per endpoint:\n   - /api/scrape: 10 requests/minute (scraping is expensive)\n   - /dashboard/*: 60 requests/minute (UI interactions)\n5. Return 429 Too Many Requests with Retry-After header\n6. Add rate limit headers to responses (X-RateLimit-Remaining, X-RateLimit-Reset)\n7. Apply middleware to scrapeRouter and dashboardRouter\n8. Add configuration options via config.ts for limit customization\n9. Write tests for rate limit enforcement and reset behavior\n\n## Acceptance Criteria\n- Requests exceeding limit receive 429 response\n- Rate limit headers present on all responses\n- Limits are configurable via environment variables\n- Legitimate usage patterns are not impacted\n- Tests verify limit enforcement and window reset","status":"tombstone","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:41.803170249+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.630125632+01:00","close_reason":"Fixed: Implemented rate limiting middleware with 10 req/min for API and 60 req/min for dashboard. Includes rate limit headers and 429 responses.","labels":["critical","security"]}
{"id":"smartScraper-1ta","title":"TestDashboardRequiresAuth follows redirects - gets 200 instead of 302","description":"## Bug\n\nTestDashboardRequiresAuth test uses http.Get() which auto-follows redirects. The server correctly returns 302 to /login, but Go's HTTP client follows it and gets 200 from the login page.\n\n## Fix\nEither:\n1. Disable redirect following in the test client\n2. Or check that final URL is /login\n\nThe application is working correctly - this is a test bug.","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T14:01:17.104297391+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.631566347+01:00"}
{"id":"smartScraper-1zd","title":"Change queue concurrency from 5 to 1 in CoreScraperEngine","description":"Update src/core/engine.ts:\n- Line 23: Change PQueue({ concurrency: 5 }) to PQueue({ concurrency: 1 })\n- Line 41-43: Change getMaxWorkers() to return 1 instead of 5\n\nThis aligns implementation with ADR-001 and ADR-003 which mandate sequential execution.\n\nSee: docs/MIGRATION-SEQUENTIAL-EXECUTION.md","status":"closed","priority":1,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:47:17.619409361+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:54:01.188180071+01:00","closed_at":"2026-02-05T14:54:01.188180071+01:00","close_reason":"Changed PQueue concurrency from 5 to 1 and getMaxWorkers() return value from 5 to 1. Updated tests to verify sequential execution."}
{"id":"smartScraper-21a","title":"Review findings for smartScraper (abbba87c): roborev show 11 / one-shot fix with roborev fix 11","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T13:02:43.084823049+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.632938199+01:00"}
{"id":"smartScraper-24v","title":"Review findings for smartScraper (21093b39): roborev show 19 / one-shot fix with roborev fix 19","status":"open","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T15:01:01.292509677+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T15:01:01.292509677+01:00"}
{"id":"smartScraper-2gg","title":"2Captcha Polling Continues After Fatal Errors","description":"solveDataDome does not detect all fatal 2Captcha API errors.\n\n## Problem\nError detection at lines 136-138 checks for status=error OR non-zero errorId, but some fatal conditions may have status=processing with an errorCode field. This causes wasted polling iterations and delayed failure responses.\n\n## Location\n- src/adapters/twocaptcha.ts:116-139\n\n## Risk\nWasted API calls (cost), delayed user feedback, potential timeout before error is properly reported.\n\n## Action Plan\n1. Research 2Captcha API documentation for all possible error response formats\n2. Identify fatal error codes that should terminate polling immediately:\n   - ERROR_CAPTCHA_UNSOLVABLE\n   - ERROR_WRONG_CAPTCHA_ID\n   - ERROR_BAD_TOKEN_OR_PAGEURL\n   - etc.\n3. Update polling loop condition to check:\n   - status === 'error'\n   - errorId !== 0\n   - presence of errorCode field (regardless of status)\n4. Extract error checking into helper function: isFatalError(response): boolean\n5. Add specific error messages for known fatal codes\n6. Add test cases with mocked 2Captcha responses for each error type\n\n## Acceptance Criteria\n- All fatal 2Captcha errors terminate polling immediately\n- Specific error messages returned for known error codes\n- No unnecessary polling iterations after fatal error\n- Tests cover all documented error response formats","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:14.749452616+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.634364321+01:00","close_reason":"Fixed: Enhanced error detection to check for errorCode field regardless of status. Added mapping for known fatal error codes to provide better error messages and terminate polling immediately.","labels":["medium","quality"]}
{"id":"smartScraper-30m","title":"Review findings for smartScraper (208cff3d): roborev show 2 / one-shot fix with roborev fix 2","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:38:54.59864411+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.635700174+01:00","close_reason":"Fixed typo: 'to that' -\u003e 'so that' in AGENTS.md workflow pattern"}
{"id":"smartScraper-4oo","title":"Add DEBUG env var support for verbose console logging","description":"Add DEBUG environment variable support throughout the codebase:\n\n1. Respect DEBUG= env var (e.g., DEBUG=1 or DEBUG=true)\n2. When enabled, spam console with verbose debug output:\n   - Timing information for each operation\n   - Browser lifecycle events\n   - XPath evaluation details\n   - Queue status changes\n   - All HTTP requests/responses\n\n3. Use DEBUG=1 for all test runs (both Vitest and E2E)\n\n4. Update test-orchestrator to pass DEBUG=1 to Hono workers\n\nFiles to update:\n- src/utils/logger.ts - check DEBUG env var\n- src/adapters/puppeteer-browser.ts - add verbose logging\n- src/core/engine.ts - add verbose logging\n- test-orchestrator/worker.go - pass DEBUG=1 to Hono\n\nThis helps diagnose test failures and production issues.","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:50:32.280197672+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T15:01:09.01409352+01:00","closed_at":"2026-02-05T15:01:09.01409352+01:00","close_reason":"DEBUG env var support was implemented in commit f82d943. DEBUG=1 or DEBUG=true now overrides LOG_LEVEL to DEBUG."}
{"id":"smartScraper-4ud","title":"Remove timing debug instrumentation from puppeteer-browser.ts","description":"Remove the [TIMING] log statements added during debugging investigation.\n\nFile: src/adapters/puppeteer-browser.ts\n\nThese were temporary instrumentation for diagnosing test timeouts.\n\nDepends on: smartScraper-1zd","status":"closed","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:47:46.455100984+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:49:59.310934884+01:00","closed_at":"2026-02-05T14:49:59.310934884+01:00","close_reason":"Removed timing instrumentation from puppeteer-browser.ts"}
{"id":"smartScraper-5kf","title":"No CSRF Protection on Dashboard Forms","description":"Dashboard POST endpoints lack CSRF protection tokens.\n\n## Problem\nPOST endpoints like /dashboard/sites/:domain do not implement CSRF tokens. HTMX requests from authenticated sessions are vulnerable to cross-site request forgery attacks.\n\n## Location\n- src/routes/dashboard/sites.tsx:269-360\n\n## Risk\nAttackers could trick authenticated users into performing unwanted actions (deleting sites, modifying configurations) via malicious links or embedded forms on third-party sites.\n\n## Action Plan\n1. Create src/middleware/csrf.ts with csrfMiddleware\n2. On GET requests: generate crypto.randomUUID() token, store in cookie (httpOnly: false for JS access)\n3. Set token in Hono context: c.set('csrfToken', token)\n4. On POST/PUT/DELETE: validate X-CSRF-Token header matches cookie\n5. Return 403 Forbidden if validation fails\n6. Update layout.tsx to include CSRF token in a meta tag\n7. Configure HTMX to send X-CSRF-Token header on all requests via hx-headers\n8. Apply csrfMiddleware to dashboardRouter\n9. Write tests for token generation, validation success, and validation failure\n\n## Acceptance Criteria\n- All dashboard POST requests require valid CSRF token\n- Missing or invalid token returns 403 response\n- HTMX requests automatically include token via configured headers\n- Token regenerates on each page load\n- Tests cover happy path and attack scenarios","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:49.772971002+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.637045312+01:00","close_reason":"Fixed: Implemented CSRF middleware with token generation on GET and validation on POST/PUT/DELETE. HTMX configured to send X-CSRF-Token header automatically.","labels":["high","security"]}
{"id":"smartScraper-6ax","title":"Missing Error Handling in Async Operations","description":"Empty catch blocks swallow errors silently.\n\n## Problem\nSeveral async operations use empty catch blocks, hiding critical errors from logs and monitoring. This makes debugging difficult and can mask underlying issues.\n\n## Location\n- src/adapters/puppeteer-browser.ts:33-35 (closePage cleanup)\n- src/core/engine.ts:296-300 (finally block cleanup)\n\n## Risk\nErrors go undetected, making debugging difficult. Potential resource leaks if cleanup fails silently. Production issues may be impossible to diagnose.\n\n## Action Plan\n1. Identify all empty catch blocks in codebase: `rg 'catch.*\\{\\s*\\}' src/`\n2. For each empty catch, determine appropriate handling:\n   - Expected failures (e.g., closing already-closed page): log at DEBUG level\n   - Unexpected failures: log at WARN level with error details\n3. Update puppeteer-browser.ts:33-35:\n   ```typescript\n   try { await session.page.close(); } \n   catch (e) { logger.debug('Page close failed (may already be closed)', { error: String(e) }); }\n   ```\n4. Update engine.ts:296-300 similarly\n5. Review other catch blocks for completeness\n6. Add eslint rule to prevent empty catch blocks: no-empty\n\n## Acceptance Criteria\n- No empty catch blocks remain in codebase\n- All caught errors are logged with appropriate level\n- Expected failures logged at DEBUG (not noisy)\n- Unexpected failures logged at WARN (visible)\n- ESLint configured to prevent future empty catches","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:17.187347472+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.63843192+01:00","close_reason":"Fixed: Added proper error logging to empty catch blocks in puppeteer-browser.ts and engine.ts. Errors are logged at DEBUG level since they represent expected cleanup failures.","labels":["medium","quality"]}
{"id":"smartScraper-6gm","title":"stats-storage.ts crashes on undefined domainCounts","description":"## Bug\n\ngetTopDomains() in stats-storage.ts:79 calls Object.entries(stats.domainCounts) which crashes with TypeError when domainCounts is undefined/null.\n\n## Stack Trace\n```\nTypeError: Cannot convert undefined or null to object\n    at Object.entries (\u003canonymous\u003e)\n    at \u003canonymous\u003e (/home/chuck/git/smartScraper/src/services/stats-storage.ts:79:19)\n```\n\n## Root Cause\nloadStatsInternal() reads stats.json and casts to Stats type without validation. If the file is missing domainCounts field, it remains undefined.\n\n## Fix\nAdd defensive check in getTopDomains() before calling Object.entries():\n```typescript\nreturn Object.entries(stats.domainCounts || {})\n```\n\nOr ensure loadStatsInternal() always returns valid Stats with domainCounts initialized.","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T13:55:47.887524298+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.639784441+01:00","close_reason":"Fixed: Added defensive check for undefined domainCounts in getTopDomains() and loadStatsInternal()"}
{"id":"smartScraper-8jd","title":"Update dashboard UI to reflect single-runner model","description":"Update dashboard to remove 'X of 5 Workers' display:\n\nFiles to update:\n- src/routes/dashboard/index.tsx\n- src/routes/dashboard/events.ts (SSE)\n- Related components\n\nChanges:\n- Replace 'X of 5 Workers' with 'Idle' / 'Scraping: {url}'\n- Simplify SSE events (no need for array of URLs)\n- Update worker status HTML fragments\n\nDepends on: smartScraper-1zd\nSee: docs/MIGRATION-SEQUENTIAL-EXECUTION.md","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:47:27.577594115+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T15:00:48.002298991+01:00","closed_at":"2026-02-05T15:00:48.002298991+01:00","close_reason":"Updated dashboard UI: replaced 'X of 5 Workers' with 'Idle' / 'Scraping: {url}' display","dependencies":[{"issue_id":"smartScraper-8jd","depends_on_id":"smartScraper-1zd","type":"blocks","created_at":"2026-02-05T14:47:50.730051869+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-92n","title":"Information Disclosure in Error Messages","description":"Error responses expose internal implementation details.\n\n## Problem\nSeveral error responses leak stack traces, file paths, and internal state that could aid attackers in reconnaissance. Examples include raw error.message from exceptions and detailed 2Captcha API responses.\n\n## Location\n- src/routes/api/scrape.ts\n- src/adapters/twocaptcha.ts\n- src/core/engine.ts\n\n## Risk\nInformation leakage aids attackers in understanding system architecture, identifying vulnerable components, and crafting targeted attacks.\n\n## Action Plan\n1. Create src/utils/error-sanitizer.ts with sanitizeErrorForClient(error): string function\n2. Define allowlist of safe error messages to expose (e.g., 'Invalid URL', 'Rate limit exceeded')\n3. Map internal errors to generic client-facing messages\n4. Update scrape.ts error handler to use sanitized messages in JSON response\n5. Keep detailed error logging internal via logger.error()\n6. Review twocaptcha.ts responses - strip API-specific details\n7. Review engine.ts - ensure stack traces never reach client\n8. Add error response tests verifying no internal details leak\n\n## Acceptance Criteria\n- Client-facing error responses contain only generic, safe messages\n- Stack traces never appear in API responses\n- Internal file paths never appear in API responses\n- Detailed errors are logged server-side for debugging\n- Tests verify error sanitization","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:59.321813794+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.641230038+01:00","close_reason":"Fixed: Created error-sanitizer.ts with sanitizeErrorForClient() function. Updated scrape endpoint to catch errors and return sanitized messages to clients while logging full details internally.","labels":["medium","security"]}
{"id":"smartScraper-9pg","title":"Review findings for smartScraper (68e8e864): roborev show 3 / one-shot fix with roborev fix 3","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:49:02.870212754+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.642647258+01:00","close_reason":"Reviewed: 5 low/medium improvement suggestions. Type safety, test coverage gaps, and CHANGELOG update are valid points for future enhancement. No blocking issues."}
{"id":"smartScraper-a0h","title":"Review findings for smartScraper (d3de60b0): roborev show 17 / one-shot fix with roborev fix 17","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:04:35.075764941+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:07:17.848472102+01:00","deleted_at":"2026-02-05T14:07:17.848472102+01:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
{"id":"smartScraper-ag3","title":"Review findings for smartScraper (f82d9435): roborev show 18 / one-shot fix with roborev fix 18","status":"open","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T14:57:37.406294951+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:57:37.406294951+01:00"}
{"id":"smartScraper-ahl","title":"Potential ReDoS in DOM Simplification","description":"simplifyDom function uses regex patterns vulnerable to catastrophic backtracking.\n\n## Problem\nRegex patterns like `\u003ctag[^\u003e]*\u003e.*?\u003c/tag\u003e` with gis flags on untrusted HTML can cause exponential backtracking with crafted nested structures. The .*? quantifier combined with alternation in complex HTML can trigger ReDoS.\n\n## Location\n- src/utils/dom.ts:8-38\n\n## Risk\nCPU exhaustion via crafted HTML input. Severity is Medium because input comes from scraped pages (not direct user input), but a malicious target site could exploit this.\n\n## Action Plan\n1. Add MAX_HTML_SIZE constant (1MB) at top of file\n2. Truncate input at start of simplifyDom: `if (html.length \u003e MAX_HTML_SIZE) html = html.slice(0, MAX_HTML_SIZE)`\n3. Consider replacing regex approach with DOM-based parsing:\n   - Use linkedom or happy-dom to parse HTML\n   - Use querySelectorAll to find and remove unwanted tags\n   - Use DOM API for class-based removal\n4. If keeping regex, add execution timeout wrapper\n5. Benchmark both approaches for performance comparison\n6. Add test cases with large HTML inputs and nested structures\n\n## Acceptance Criteria\n- HTML inputs over 1MB are truncated before processing\n- Processing completes in bounded time regardless of input structure\n- Existing functionality preserved for normal inputs\n- Performance benchmarks show acceptable overhead\n- No regression in LLM prompt generation quality","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:53.417945548+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.64431424+01:00","close_reason":"Fixed: Added MAX_HTML_SIZE constant (1MB) to truncate oversized inputs before regex processing, preventing ReDoS attacks via crafted HTML.","labels":["medium","security"]}
{"id":"smartScraper-b9a","title":"Fix server crash on benign Puppeteer extension cleanup error","description":"## Bug Report: Server Crash on Extension Cleanup\n\n**Problem Summary**\nThe application crashes with `TargetCloseError: Protocol error (Extensions.loadUnpacked): Target closed` during normal operation. This happens because the global `unhandledRejection` handler triggers `process.exit(1)` for *all* unhandled errors, including benign cleanup race conditions.\n\n**Detailed Analysis**\n1. **Trigger:** `PuppeteerBrowserAdapter` launches a new isolated browser for each scrape.\n2. **Race Condition:** Browser extensions start loading asynchronously. If the scrape finishes or fails fast (before extensions fully load), the application calls `browser.close()`.\n3. **Rejection:** The background extension loading promise fails because the target (browser) is closed. It rejects with `TargetCloseError`.\n4. **Crash:** This rejection bubbles up to `src/index.ts`. The strict global handler catches it and kills the server.\n\n**Compliance Note**\nThe application correctly follows the \"No Shared Browser\" rule (each request gets a new `userDataDir` and browser instance). The crash is purely due to overly aggressive global error handling catching a benign lifecycle error.\n\n**Required Fix**\nModify `src/index.ts` to add exception logic to the `unhandledRejection` handler.\n\n**Implementation Plan**\nIn `src/index.ts`:\n1. Check if `reason.message` (or `reason.toString()`) includes \"TargetCloseError\" AND \"Extensions.loadUnpacked\".\n2. If matched: Log as `WARN` and **do not exit**.\n3. Else: Log as `FATAL` and `process.exit(1)` (preserve existing Fail Fast behavior).\n\n**Why this is safe**\nThis error only occurs during the *teardown* of an isolated browser instance. It does not affect the global state of the server or other concurrent requests. Ignoring it is safe and necessary for stability.","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T13:42:32.49665184+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.645935383+01:00","close_reason":"Fixed: Modified unhandledRejection handler to detect and ignore benign TargetCloseError from Extensions.loadUnpacked"}
{"id":"smartScraper-bh9","title":"console.log Used Instead of Logger","description":"twocaptcha.ts uses console.log directly instead of centralized logger.\n\n## Problem\nLines 102, 109, and 126 use console.log for debug output instead of the project's logger utility. This violates src/AGENTS.md convention and bypasses log level filtering, structured logging, and any log aggregation setup.\n\n## Location\n- src/adapters/twocaptcha.ts:102, 109, 126\n\n## Risk\nLow severity but poor code hygiene. Debug output may appear in production. Inconsistent log format makes parsing difficult.\n\n## Action Plan\n1. Import logger at top of file: `import { logger } from '../utils/logger.js';`\n2. Replace line 102 console.log with logger.debug\n3. Replace line 109 console.log with logger.debug\n4. Replace line 126 console.log with logger.debug\n5. Use structured logging format: `logger.debug('message', { data }, 'CAPTCHA')`\n6. Ensure sensitive data (API keys) remain redacted\n7. Run grep to verify no console.log remains: `rg 'console\\.log' src/`\n\n## Acceptance Criteria\n- No console.log calls in twocaptcha.ts\n- All logging uses project logger utility\n- Log level is DEBUG (not INFO) for verbose API interaction logs\n- API keys remain redacted in logs\n- Grep confirms no console.log in src/","status":"tombstone","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:20.349170684+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.647545601+01:00","close_reason":"Fixed: Replaced all console.log calls with logger.debug() in twocaptcha.ts. API keys remain properly redacted in log output.","labels":["low","quality"]}
{"id":"smartScraper-bt9","title":"Implement Go Test Orchestrator for SmartScraper","description":"Complete rewrite of testing protocol using a Go-based parallel test orchestrator adapted from Artwalls.\n\n## Overview\nPort the Artwalls Go test orchestrator to SmartScraper with the following adaptations:\n- Replace Neon database branching with ephemeral DATA_DIR directories\n- Replace Wrangler dev servers with Hono Node.js instances\n- Adapt domain models for SmartScraper (ScrapeResult, SiteConfig, Stats)\n- Maintain parallel execution, health checking, and test caching\n\n## Key Components\n1. Core orchestrator (main.go, orchestrator.go, errors.go)\n2. Worker management (worker.go, workerpool.go)\n3. Isolation management (isolation.go - new, replaces Neon branching)\n4. Infrastructure (tmux.go, health.go, secrets.go, cache.go)\n5. Just commands integration\n6. Comprehensive E2E test suite\n\n## Success Criteria\n- [ ] `just test` runs parallel tests with 4 workers\n- [ ] Each worker has isolated DATA_DIR\n- [ ] Health checks pass before tests run\n- [ ] Test caching skips unchanged passing tests\n- [ ] Cleanup removes ephemeral directories\n- [ ] All existing API endpoints have E2E coverage","status":"tombstone","priority":1,"issue_type":"epic","owner":"bogorad@gmail","created_at":"2026-02-05T12:35:05.111717066+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.649098481+01:00","close_reason":"Go test orchestrator implementation complete with all 12 subtasks finished"}
{"id":"smartScraper-bt9.1","title":"Create errors.go sentinel errors","description":"Create test-orchestrator/errors.go with sentinel errors for the orchestrator.\n\n## Action Plan\n1. Create errors.go with sentinel error definitions\n2. Include errors for: secret decryption, isolation, tmux, health check, test execution, worker management\n\n## Errors to Define\n- ErrSecretDecryptFailed\n- ErrIsolationCreateFailed\n- ErrIsolationCleanupFailed\n- ErrTmuxStartFailed\n- ErrTmuxCommandFailed\n- ErrHealthCheckTimeout\n- ErrTestsFailed\n- ErrNoTestsFound\n- ErrWorkerNotAvailable\n- ErrWorkerStartFailed\n\n## Acceptance Criteria\n- [ ] errors.go compiles without errors\n- [ ] All sentinel errors are exported","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:35:15.313936326+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.650488648+01:00","close_reason":"Created errors.go with sentinel errors","dependencies":[{"issue_id":"smartScraper-bt9.1","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:35:15.316582086+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.10","title":"Create main.go CLI entry point","description":"Create test-orchestrator/main.go CLI entry point.\n\n## Action Plan\n1. Implement main() with:\n   - Panic recovery\n   - Working directory detection (change to project root if in test-orchestrator/)\n   - Flag parsing\n   - Context with cancellation\n   - Signal handling (SIGINT, SIGTERM)\n   - Orchestrator creation and run\n   - Cleanup on exit\n2. Implement parseFlags() returning Config with:\n   - --file/-f: pattern filter\n   - --full: bypass cache\n   - --workers/-w: worker count (1-8)\n   - --timeout: health check timeout\n   - --verbose/-v: verbose output\n   - --logs-dir: log directory\n\n## Port from Artwalls\n- Copy main.go structure directly\n- Update working directory detection for SmartScraper paths\n\n## Acceptance Criteria\n- [ ] main.go compiles\n- [ ] `go run .` starts orchestrator\n- [ ] All flags work correctly\n- [ ] Graceful shutdown on SIGINT","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:56.379345046+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.651855404+01:00","close_reason":"Created main.go","dependencies":[{"issue_id":"smartScraper-bt9.10","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:56.382021222+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.10","depends_on_id":"smartScraper-bt9.9","type":"blocks","created_at":"2026-02-05T12:36:56.385219098+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.11","title":"Update .justfile with test commands","description":"Add test orchestrator commands to .justfile.\n\n## Action Plan\n1. Add 'test' recipe: `cd test-orchestrator \u0026\u0026 go run . --workers 4`\n2. Add 'test-file' recipe with pattern parameter\n3. Add 'test-full' recipe with --full flag\n4. Add 'test-clean' recipe to clean cache, logs, temp dirs, orphan processes\n\n## Recipes to Add\n```just\n# Run tests with parallel workers\ntest:\n    cd test-orchestrator \u0026\u0026 go run . --workers 4\n\n# Run specific test by pattern\ntest-file pattern:\n    cd test-orchestrator \u0026\u0026 go run . --workers 1 --file {{pattern}}\n\n# Force full test run (bypass cache)\ntest-full:\n    cd test-orchestrator \u0026\u0026 go run . --workers 4 --full\n\n# Clean up orphan processes and cache\ntest-clean:\n    rm -f .test-cache.json\n    rm -rf test-orchestrator/logs/*\n    rm -rf /tmp/smartscraper-test-*\n    for port in 9000 9001 9002 9003 9004 9005 9006 9007; do \\\n        lsof -ti:$port | xargs -r kill -9 2\u003e/dev/null || true; \\\n    done\n```\n\n## Acceptance Criteria\n- [ ] `just test` runs orchestrator\n- [ ] `just test-file TestHealth` filters tests\n- [ ] `just test-full` bypasses cache\n- [ ] `just test-clean` removes all test artifacts","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:37:07.442529899+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.653192639+01:00","close_reason":"Implementation complete","dependencies":[{"issue_id":"smartScraper-bt9.11","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:37:07.445252039+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.11","depends_on_id":"smartScraper-bt9.10","type":"blocks","created_at":"2026-02-05T12:37:07.448679599+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.12","title":"Write comprehensive E2E test suite","description":"Expand E2E test suite with comprehensive coverage.\n\n## Action Plan\n1. Expand e2e/helpers.go with any missing utilities\n2. Create e2e/api_test.go with API endpoint tests\n3. Create e2e/dashboard_test.go with dashboard tests\n4. Create e2e/auth_test.go with authentication tests\n5. Create e2e/storage_test.go with file storage tests\n6. Ensure all tests follow patterns from AGENTS.md\n\n## Test Files to Create/Expand\n\n### api_test.go\n- TestApiScrapeSuccess (with known config)\n- TestApiScrapeDiscovery (without known config, uses LLM)\n- TestApiScrapeOutputTypes (content_only, full_html, metadata_only)\n- TestApiScrapeStats (verifies stats.json increment)\n- TestApiScrapeLogs (verifies log entry created)\n\n### dashboard_test.go\n- TestDashboardSitesRenders\n- TestDashboardSitesFiltering\n- TestDashboardSiteDetail\n- TestDashboardSiteUpdate\n- TestDashboardSiteDelete\n- TestDashboardStats\n\n### auth_test.go\n- TestApiRequiresAuth\n- TestApiRejectsInvalidToken\n- TestDashboardRequiresAuth\n- TestDashboardSessionCookie\n\n### storage_test.go\n- TestSitesFilePersistence\n- TestStatsFileIncrement\n- TestLogFileCreation\n\n## Acceptance Criteria\n- [ ] All test files compile\n- [ ] Tests pass with `go test ./e2e/...` (with proper env vars)\n- [ ] At least 15 distinct test functions\n- [ ] Coverage of all major endpoints","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:37:21.115021298+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.654633825+01:00","close_reason":"Implementation complete","dependencies":[{"issue_id":"smartScraper-bt9.12","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:37:21.118233326+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.12","depends_on_id":"smartScraper-bt9.10","type":"blocks","created_at":"2026-02-05T12:37:21.122066002+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.2","title":"Create secrets.go for SOPS decryption","description":"Create test-orchestrator/secrets.go to load secrets from SOPS-encrypted secrets.yaml.\n\n## Action Plan\n1. Create Secrets struct with SmartScraper-specific fields (API_TOKEN, OPENROUTER_API_KEY, TWOCAPTCHA_API_KEY, PROXY_SERVER)\n2. Implement LoadSecrets() using `sops -d secrets.yaml`\n3. Implement Env() method to return KEY=value strings\n4. Support fallback to environment variables for CI\n\n## Key Differences from Artwalls\n- No Neon-related secrets (NEON_API_KEY, NEON_PROJECT_ID, etc.)\n- SmartScraper secrets: API_TOKEN, OPENROUTER_API_KEY, TWOCAPTCHA_API_KEY, PROXY_SERVER\n\n## Acceptance Criteria\n- [ ] secrets.go compiles\n- [ ] Can load from environment variables (CI mode)\n- [ ] Can decrypt via sops (dev mode)","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:35:27.071269804+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.655921919+01:00","close_reason":"Created Go source files","dependencies":[{"issue_id":"smartScraper-bt9.2","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:35:27.074254516+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.2","depends_on_id":"smartScraper-bt9.1","type":"blocks","created_at":"2026-02-05T12:35:27.077254888+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.3","title":"Create isolation.go for ephemeral DATA_DIR management","description":"Create test-orchestrator/isolation.go to manage ephemeral data directories (replaces Neon branching).\n\n## Action Plan\n1. Create IsolatedEnv struct with fields: ID, DataDir, SitesFile, StatsFile, LogsDir, Port\n2. Implement CreateIsolatedEnv(workerID int) to create temp directory structure\n3. Implement SeedDefaultFiles() to initialize sites.jsonc, stats.json\n4. Implement Cleanup() to remove the temp directory\n5. Implement Env() to return environment variables for the worker\n\n## Directory Structure Created\n```\n/tmp/smartscraper-test-{workerID}-{timestamp}/\n├── sites.jsonc      # Empty array: []\n├── stats.json       # Empty object: {}\n└── logs/            # Empty directory\n```\n\n## Environment Variables Returned\n- DATA_DIR=/tmp/smartscraper-test-{id}-{ts}/\n- PORT=900{id}\n- API_TOKEN=test-token-{id}\n\n## Acceptance Criteria\n- [ ] isolation.go compiles\n- [ ] CreateIsolatedEnv creates directory structure\n- [ ] Cleanup removes directory\n- [ ] Multiple isolated envs can coexist","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:35:42.311407488+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.657270104+01:00","close_reason":"Created Go source files","dependencies":[{"issue_id":"smartScraper-bt9.3","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:35:42.314095487+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.3","depends_on_id":"smartScraper-bt9.1","type":"blocks","created_at":"2026-02-05T12:35:42.316991842+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.4","title":"Create tmux.go for session management","description":"Create test-orchestrator/tmux.go to manage tmux sessions for workers.\n\n## Action Plan\n1. Define TmuxSocketDir constant (/tmp/claude-tmux-sockets)\n2. Create TmuxSession struct with Socket and Name fields\n3. Implement NewTmuxSession(workerID int)\n4. Implement EnsureSocketDir()\n5. Implement Start(verbose bool) - create new detached session\n6. Implement SendCommand(command string, verbose bool)\n7. Implement Kill() - terminate session\n8. Implement Exists() - check if session running\n9. Implement CapturePaneOutput(lines int) for debugging\n\n## Port from Artwalls\n- Copy tmux.go structure directly\n- No SmartScraper-specific changes needed\n\n## Acceptance Criteria\n- [ ] tmux.go compiles\n- [ ] Can create and kill tmux sessions\n- [ ] Can send commands to sessions\n- [ ] Can capture pane output","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:35:52.89846925+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.658691645+01:00","close_reason":"Created Go source files","dependencies":[{"issue_id":"smartScraper-bt9.4","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:35:52.901659098+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.4","depends_on_id":"smartScraper-bt9.1","type":"blocks","created_at":"2026-02-05T12:35:52.904728581+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.5","title":"Create health.go for HTTP health checking","description":"Create test-orchestrator/health.go to poll /health endpoints.\n\n## Action Plan\n1. Create HealthChecker struct with URL, Timeout, backoff config\n2. Implement NewHealthChecker(url string, timeout time.Duration)\n3. Implement Wait(ctx context.Context) with exponential backoff\n4. Implement WaitWithProgress(ctx, onAttempt func) for progress reporting\n\n## Port from Artwalls\n- Copy health.go structure directly\n- No SmartScraper-specific changes needed\n\n## Acceptance Criteria\n- [ ] health.go compiles\n- [ ] Polls URL until 200 response\n- [ ] Respects timeout\n- [ ] Uses exponential backoff","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:00.328062785+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.660158256+01:00","close_reason":"Created Go source files","dependencies":[{"issue_id":"smartScraper-bt9.5","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:00.330561816+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.5","depends_on_id":"smartScraper-bt9.1","type":"blocks","created_at":"2026-02-05T12:36:00.333835772+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.6","title":"Create cache.go for test result caching","description":"Create test-orchestrator/cache.go for smart test caching.\n\n## Action Plan\n1. Create TestCacheEntry struct with Hash, LastRun, Passed\n2. Create TestCache struct with Entries map and HelpersHash\n3. Implement LoadCache() to read .test-cache.json\n4. Implement Save() to persist cache\n5. Implement NeedsRun(filePath) to check if test should run\n6. Implement MarkPassed/MarkFailed to record results\n7. Implement CheckHelpersChanged() to invalidate all on helpers.go change\n8. Implement FileHash(path) using MD5\n\n## Port from Artwalls\n- Copy cache.go structure directly\n- Update helpers path to test-orchestrator/e2e/helpers.go\n\n## Acceptance Criteria\n- [ ] cache.go compiles\n- [ ] Can load/save cache file\n- [ ] Correctly detects file changes\n- [ ] Invalidates all when helpers.go changes","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:10.555737636+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.661556985+01:00","close_reason":"Created Go source files","dependencies":[{"issue_id":"smartScraper-bt9.6","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:10.560373883+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.6","depends_on_id":"smartScraper-bt9.1","type":"blocks","created_at":"2026-02-05T12:36:10.564129275+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.7","title":"Create worker.go for individual worker lifecycle","description":"Create test-orchestrator/worker.go to manage individual test workers.\n\n## Action Plan\n1. Define WorkerStatus enum (Idle, Starting, Ready, Running, Stopping, Failed)\n2. Create Worker struct with: ID, Port, TmuxSession, IsolatedEnv, LogFile, Status\n3. Implement NewWorker(id int, logsDir string)\n4. Implement StartTmux(verbose bool) - start tmux session only\n5. Implement SetupIsolation() - create ephemeral DATA_DIR\n6. Implement StartHono(verbose bool) - send npm run dev command to tmux\n7. Implement CheckHealth(ctx) - poll /health endpoint\n8. Implement WaitReady(ctx, timeout, verbose) - wait for healthy\n9. Implement Stop(ctx) - kill tmux, cleanup isolation\n10. Implement CaptureLogs(lines int)\n11. Implement URL() - return base URL\n12. Implement Env() - return environment variables\n\n## Key Differences from Artwalls\n- Uses IsolatedEnv instead of NeonBranch\n- Starts Hono via npm run dev instead of wrangler\n- No database connection string\n\n## Acceptance Criteria\n- [ ] worker.go compiles\n- [ ] Can create and start worker\n- [ ] Worker gets isolated DATA_DIR\n- [ ] Health check works\n- [ ] Cleanup removes all resources","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:23.290398277+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.662869642+01:00","close_reason":"Created worker.go","dependencies":[{"issue_id":"smartScraper-bt9.7","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:23.293178414+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.7","depends_on_id":"smartScraper-bt9.3","type":"blocks","created_at":"2026-02-05T12:36:23.296226518+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.7","depends_on_id":"smartScraper-bt9.4","type":"blocks","created_at":"2026-02-05T12:36:23.299280914+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.7","depends_on_id":"smartScraper-bt9.5","type":"blocks","created_at":"2026-02-05T12:36:23.302507871+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.8","title":"Create workerpool.go for worker pool management","description":"Create test-orchestrator/workerpool.go to manage pool of workers.\n\n## Action Plan\n1. Create WorkerPool struct with: workers slice, maxWorkers, logsDir, available channel\n2. Implement NewWorkerPool(maxWorkers, logsDir int)\n3. Implement Start(ctx, healthTimeout, verbose) with phases:\n   - Phase 1: Start all tmux sessions in parallel\n   - Phase 2: Create ephemeral DATA_DIRs in parallel\n   - Phase 3: Start Hono on all workers in parallel\n   - Phase 4: Health check loop until all healthy or timeout\n4. Implement Acquire(ctx, timeout, verbose) - get available worker\n5. Implement Release(worker) - return to pool\n6. Implement Shutdown(ctx) - stop all workers gracefully\n7. Implement HealthCheck() - verify all workers healthy\n\n## Key Differences from Artwalls\n- No Neon branch creation (sequential API calls)\n- Parallel ephemeral dir creation (no API rate limits)\n- Faster startup expected\n\n## Acceptance Criteria\n- [ ] workerpool.go compiles\n- [ ] Can start pool of workers\n- [ ] Acquire/Release cycle works\n- [ ] Shutdown cleans up all resources","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:33.630632139+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.664246251+01:00","close_reason":"Created workerpool.go","dependencies":[{"issue_id":"smartScraper-bt9.8","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:33.633145899+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.8","depends_on_id":"smartScraper-bt9.7","type":"blocks","created_at":"2026-02-05T12:36:33.636333255+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-bt9.9","title":"Create orchestrator.go main coordination logic","description":"Create test-orchestrator/orchestrator.go for main test orchestration.\n\n## Action Plan\n1. Create Config struct with: MaxWorkers, Pattern, ForceFull, LogsDir, HealthTimeout, Verbose\n2. Create TestInfo struct with: FuncName, FilePath\n3. Create Orchestrator struct with: config, secrets, pool, cache, cleanupFuncs\n4. Implement NewOrchestrator(config) - initialize all components\n5. Implement Run(ctx) with phases:\n   - Discover test functions\n   - Filter by pattern\n   - Filter by cache\n   - Start worker pool\n   - Queue tests\n   - Run parallel tests\n   - Report results\n6. Implement discoverTestFiles() - grep for 'func Test' in e2e/*.go\n7. Implement filterTestFiles(tests, pattern)\n8. Implement filterByCache(tests)\n9. Implement runParallelTests(ctx, queue, cachedCount)\n10. Implement runSingleTest(ctx, worker, testInfo)\n11. Implement RegisterCleanup(fn)\n12. Implement Cleanup()\n\n## Key Differences from Artwalls\n- No Neon project/branch lookup\n- Different test execution command (still go test but different env vars)\n- Sets DATA_DIR instead of DATABASE_URL\n\n## Acceptance Criteria\n- [ ] orchestrator.go compiles\n- [ ] Can discover tests\n- [ ] Can filter by pattern\n- [ ] Can run tests in parallel\n- [ ] Reports pass/fail summary","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T12:36:46.659547197+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.665498796+01:00","close_reason":"Created orchestrator.go","dependencies":[{"issue_id":"smartScraper-bt9.9","depends_on_id":"smartScraper-bt9","type":"parent-child","created_at":"2026-02-05T12:36:46.662699427+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.9","depends_on_id":"smartScraper-bt9.2","type":"blocks","created_at":"2026-02-05T12:36:46.666021694+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.9","depends_on_id":"smartScraper-bt9.6","type":"blocks","created_at":"2026-02-05T12:36:46.669225568+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-bt9.9","depends_on_id":"smartScraper-bt9.8","type":"blocks","created_at":"2026-02-05T12:36:46.672269745+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-c51","title":"Fix twocaptcha.test.ts config mock missing getLogLevel","description":"## Problem\n\nThe `twocaptcha.test.ts` test file has an incomplete config mock. The mock includes CAPTCHA-specific config getters but omits `getLogLevel`, which is required by the logger utility used in the TwoCaptchaAdapter.\n\n## Failing Tests (8)\n- should delegate to solveDataDome for DataDome CAPTCHA\n- should successfully solve DataDome CAPTCHA\n- should include proxy details when provided\n- should handle task creation failure\n- should poll until task is ready\n- should handle error status from task result\n- should handle missing cookie in solution\n- should timeout when DataDome solving takes too long\n\n## Root Cause\nThe TwoCaptchaAdapter uses `logger.debug()` at lines 103, 113, 130. The logger calls `getLogLevel()` which is not in the mock.\n\n## Fix\nUpdate the mock in `src/adapters/twocaptcha.test.ts`:\n```typescript\nvi.mock('../config.js', () =\u003e ({\n  getTwocaptchaApiKey: () =\u003e 'test-api-key',\n  getCaptchaDefaultTimeout: () =\u003e 120,\n  getCaptchaPollingInterval: () =\u003e 100,\n  getLogLevel: () =\u003e 'NONE'  // Add this line\n}));\n```","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:14.993560428+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.666759198+01:00","close_reason":"Added getLogLevel to config mock"}
{"id":"smartScraper-cnv","title":"Potential Memory Leak in SSE Connections","description":"SSE implementation lacks connection limits and stale cleanup.\n\n## Problem\nThe clients Set at index.tsx:44 has no maximum connection limit. No timeout mechanism exists for stale connections that fail to disconnect properly, potentially leading to memory exhaustion.\n\n## Location\n- src/routes/dashboard/index.tsx:39-163\n\n## Risk\nMemory leak if connections accumulate. Potential DoS vector if attacker opens many SSE connections.\n\n## Action Plan\n1. Add MAX_SSE_CLIENTS constant (e.g., 100)\n2. Check clients.size before adding new client in /events handler\n3. Return 503 Service Unavailable if limit exceeded\n4. Add connection timestamp to Client interface\n5. Implement periodic cleanup (every 60s) to remove connections older than 10 minutes\n6. Add heartbeat response tracking - remove clients that fail to receive heartbeat\n7. Log connection/disconnection events for monitoring\n8. Add metrics endpoint or logging for current client count\n\n## Acceptance Criteria\n- New connections rejected with 503 when limit reached\n- Stale connections cleaned up automatically\n- Memory usage bounded regardless of connection patterns\n- Monitoring visibility into connection count\n- Tests verify limit enforcement and cleanup","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:08.382732445+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.668112475+01:00","close_reason":"Fixed: Added MAX_SSE_CLIENTS limit (100), connection timeout (10 min), and periodic cleanup of stale connections to prevent memory leaks.","labels":["medium","quality"]}
{"id":"smartScraper-e2h","title":"Review findings for smartScraper (ce0058ba): roborev show 6 / one-shot fix with roborev fix 6","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:59:49.302857548+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.669436077+01:00","close_reason":"Both findings are false positives: (1) 'Fallback (also supported)' wording is accurate - precedence is already documented on line 20, (2) CHANGELOG.md does not exist so no update required per AGENTS.md rules"}
{"id":"smartScraper-g2v","title":"Insufficient Logging of Security Events","description":"Security events logged at inappropriate levels.\n\n## Problem\nFailed authentication attempts and configuration changes are logged at INFO/DEBUG level in auth.ts. Production log configurations often filter these levels, making security incident investigation difficult.\n\n## Location\n- src/middleware/auth.ts\n\n## Risk\nSecurity incidents may go unnoticed. Audit trail incomplete for compliance requirements. Difficult to detect brute-force attacks or unauthorized access attempts.\n\n## Action Plan\n1. Review all logger calls in auth.ts and identify security-relevant events\n2. Create security event categories:\n   - AUTH_FAILURE: Failed login attempts (WARN level)\n   - AUTH_SUCCESS: Successful logins (INFO level, but always captured)\n   - SESSION_CREATED: New session establishment (INFO level)\n   - SESSION_INVALID: Invalid session presented (WARN level)\n3. Update log levels:\n   - logger.warn for all failure cases (currently some use logger.info)\n   - Include relevant context: IP address, user agent, timestamp\n4. Consider structured logging format for security events\n5. Document security logging in ADR or CONFIGURATION.md\n6. Verify production log config captures WARN and above\n\n## Acceptance Criteria\n- All authentication failures logged at WARN or higher\n- Security events include sufficient context for investigation\n- Production log configuration captures all security events\n- Log format suitable for SIEM ingestion if needed","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:11.072524028+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.67066758+01:00","close_reason":"Fixed: Enhanced security event logging in auth.ts with WARN level for authentication failures including IP address and user agent context for audit trail.","labels":["medium","security"]}
{"id":"smartScraper-goi","title":"Critical: Hardcoded timeout in browser reload after CAPTCHA solving","description":"## Problem\n\nKimi Audit Issue #2 (Critical) was NOT addressed in the security fixes.\n\nThe browser reload operation after CAPTCHA solving uses a hardcoded timeout value (Puppeteer's default of 30,000 ms) instead of respecting the user-provided `timeoutMs` parameter.\n\nWhen a CAPTCHA is detected and solved, and cookies are updated, the engine calls `browser.reload(pageId)` without passing any timeout parameter. This causes operations to fail with \"Navigation timeout of 30000 ms exceeded\" even when users explicitly provide custom timeout values.\n\n## Locations\n\n- `src/core/engine.ts` line 157 - calls `reload(pageId)` without timeout\n- `src/ports/browser.ts` line 20 - interface lacks timeout option\n- `src/adapters/puppeteer-browser.ts` lines 278-282 - implementation uses Puppeteer default\n\n## Risk\n\nCAPTCHA-protected sites become inaccessible regardless of user timeout configuration, breaking core functionality.\n\n## Action Plan\n\n1. Update `BrowserPort` interface to accept optional timeout/waitUntil in `reload()`\n2. Update `PuppeteerBrowserAdapter.reload()` to use `options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Update `CoreScraperEngine` to pass `timeoutMs` from scrape options to reload call\n4. Add test coverage for timeout propagation\n\n## Fix Reference (from Kimi audit)\n\n\\`\\`\\`typescript\n// Step 1: Update BrowserPort interface\nreload(pageId: string, options?: { timeout?: number; waitUntil?: 'load' | 'domcontentloaded' | 'networkidle0' | 'networkidle2' }): Promise\u003cvoid\u003e;\n\n// Step 2: Update PuppeteerBrowserAdapter\nasync reload(pageId: string, options?: { timeout?: number; waitUntil?: string }): Promise\u003cvoid\u003e {\n  const session = this.sessions.get(pageId);\n  if (!session) return;\n  await session.page.reload({\n    waitUntil: options?.waitUntil || 'networkidle2',\n    timeout: options?.timeout || DEFAULTS.TIMEOUT_MS\n  });\n}\n\n// Step 3: Update CoreScraperEngine call\nif (solveResult.updatedCookie) {\n  await this.browserPort.setCookies(pageId, solveResult.updatedCookie);\n  await this.browserPort.reload(pageId, {\n    timeout: options?.timeoutMs || DEFAULTS.TIMEOUT_MS,\n    waitUntil: 'networkidle2'\n  });\n}\n\\`\\`\\`","status":"tombstone","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:55:39.843504235+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.672000746+01:00","close_reason":"Fixed: reload() now uses DEFAULTS.TIMEOUT_MS (120s) as fallback instead of Puppeteer default (30s) when timeoutMs is undefined. The fix was partially in place; this commit ensures consistent timeout behavior."}
{"id":"smartScraper-gzk","title":"TestDashboardSitesRenders uses Bearer token instead of session cookie","description":"## Bug\n\nTestDashboardSitesRenders uses NewTestClient() with Bearer token, but dashboard routes require session cookies (not Bearer auth). The test gets redirected to login page.\n\n## Fix\nThe test needs to:\n1. First login via POST /login with the API token\n2. Use the session cookie from the login response\n3. Then access /dashboard/sites\n\nSee TestDashboardWithSession for the correct pattern.","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T14:02:06.957789438+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.673448111+01:00"}
{"id":"smartScraper-gzs","title":"Fix auth.test.ts createSession mock context","description":"## Problem\n\nThe `auth.test.ts` test for `createSession` uses an incomplete mock Hono context. The mock only has a `header` function and `req.url`, but `createSession` requires a full Hono Context with cookie handling methods.\n\n## Failing Test (1)\n- should set session cookie\n\n## Root Cause\nThe test creates a minimal mock:\n```typescript\nconst mockContext = {\n  header: vi.fn(),\n  req: { url: 'http://localhost' }\n};\n```\n\nBut `createSession` calls Hono's `setCookie()` helper which requires proper Context implementation including the response object.\n\n## Fix\nEither:\n1. Use a real Hono test app with `app.request()` pattern (consistent with other tests in the file)\n2. Mock the full Context interface including `res` object and `set` method\n\nRecommended approach - use real Hono app:\n```typescript\nit('should set session cookie', async () =\u003e {\n  const testApp = new Hono();\n  testApp.post('/login', (c) =\u003e {\n    createSession(c, 'test-token');\n    return c.text('OK');\n  });\n\n  const req = new Request('http://localhost/login', { method: 'POST' });\n  const res = await testApp.request(req);\n  \n  expect(res.headers.get('Set-Cookie')).toContain('ss_session=');\n});\n```","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:27.344724451+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.675017799+01:00","close_reason":"Fixed createSession test to use real Hono app pattern"}
{"id":"smartScraper-iyg","title":"Review findings for smartScraper (b6032102): roborev show 15 / one-shot fix with roborev fix 15","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T13:49:25.105991899+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.676522451+01:00"}
{"id":"smartScraper-nta","title":"Fix engine.test.ts reload signature mismatch","description":"## Problem\n\nThe `engine.test.ts` test for CAPTCHA handling expects `browserPort.reload(pageId)` to be called with only one argument, but the actual implementation passes two arguments.\n\n## Failing Test (1)\n- should handle CAPTCHA detection and solving\n\n## Root Cause\nThe test expectation at line 131:\n```typescript\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123');\n```\n\nBut the actual call in `engine.ts:170`:\n```typescript\nawait this.browserPort.reload(pageId, options?.timeoutMs || DEFAULTS.TIMEOUT_MS);\n```\n\nThe reload method signature includes an optional timeout parameter that was added after the test was written.\n\n## Fix\nUpdate the test expectation in `src/core/engine.test.ts`:\n```typescript\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123', expect.any(Number));\n// Or be more specific:\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123', 120000);\n```","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:40.518404995+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.677844771+01:00","close_reason":"Fixed reload expectation to include timeout parameter"}
{"id":"smartScraper-nvh","title":"Review findings for smartScraper (f647484b): roborev show 5 / one-shot fix with roborev fix 5","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:56:12.440629282+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.679266601+01:00","close_reason":"Reviewed 5 low findings: 3 false positives dismissed (error field matches ScrapeResult, npm syntax correct, no CHANGELOG.md), 2 fixes applied (extensionPaths context in ADR-001, 'Legacy' to 'Fallback' in ADR-007)"}
{"id":"smartScraper-nw5","title":"XPath Injection Vulnerability","description":"evaluateXPath function passes user-controlled XPath expressions directly to page.evaluate() without validation.\n\n## Problem\nNo validation or sanitization of XPath input allows:\n1. CPU exhaustion via computationally expensive queries (e.g., deeply nested predicates)\n2. Memory exhaustion via queries returning massive result sets\n3. Potential information disclosure via reading unexpected document nodes\n\n## Location\n- src/adapters/puppeteer-browser.ts:152-188\n\n## Risk\nResource exhaustion attacks from authenticated users. Requires valid API token, so severity is High rather than Critical.\n\n## Action Plan\n1. Define allowed XPath character pattern: `/^[\\w\\-\\/\\[\\]@=\"'\\s\\.\\(\\)\\|\\*\\:]+$/`\n2. Set maximum XPath length constant: `MAX_XPATH_LENGTH = 500`\n3. Create validateXPath(xpath: string): boolean function\n4. Add validation call at start of evaluateXPath method\n5. Return null and log warning for invalid XPaths\n6. Add timeout protection inside page.evaluate to limit execution time\n7. Write test cases for valid XPaths, malformed XPaths, and oversized XPaths\n\n## Acceptance Criteria\n- Invalid XPaths are rejected before reaching Puppeteer\n- Oversized XPaths (\u003e500 chars) are rejected\n- Valid XPaths continue to work correctly\n- Rejection is logged with truncated XPath for debugging\n- All existing tests pass","status":"tombstone","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:38.726979396+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.680585839+01:00","close_reason":"Fixed: Added XPath validation with max length (500 chars) and allowed character pattern to prevent injection attacks.","labels":["high","security"]}
{"id":"smartScraper-ppp","title":"Review findings for smartScraper (1a3f893f): roborev show 16 / one-shot fix with roborev fix 16","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T13:50:03.821639604+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.681946245+01:00"}
{"id":"smartScraper-t4i","title":"Review findings for smartScraper (6fd9f612): roborev show 1 / one-shot fix with roborev fix 1","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:38:44.941918897+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.683310437+01:00","close_reason":"Reviewed: 6 low/medium findings, mostly improvement suggestions. Rate-limit race condition not applicable (single-threaded JS). Test coverage and error type improvements can be tracked separately."}
{"id":"smartScraper-ubh","title":"Inconsistent Default Timeout Values","description":"PuppeteerBrowserAdapter uses inconsistent timeout defaults across operations.\n\n## Problem\n- Browser launch uses DEFAULTS.TIMEOUT_MS (120000ms) at line 77\n- Page navigation uses hardcoded 45000ms at line 105\nThis violates DRY principle and causes non-deterministic timeout behavior. Users setting custom timeouts may be confused when navigation uses different defaults than other operations.\n\n## Location\n- src/adapters/puppeteer-browser.ts:77, 105\n\n## Risk\nFunctional bugs where navigation times out unexpectedly, or operations take longer than expected. Violates principle of least surprise.\n\n## Action Plan\n1. Locate all hardcoded timeout values in puppeteer-browser.ts\n2. Replace line 105 timeout: `timeout: options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Search for other hardcoded timeouts in the file (reload, waitForSelector, etc.)\n4. Ensure all timeout parameters consistently use DEFAULTS.TIMEOUT_MS as fallback\n5. Consider adding DEFAULTS.NAVIGATION_TIMEOUT if navigation needs different default\n6. Update any related documentation in ADR-001\n7. Add test to verify timeout propagation from options to Puppeteer calls\n\n## Acceptance Criteria\n- All timeout parameters use DEFAULTS.TIMEOUT_MS consistently\n- Custom timeout options are respected throughout the call chain\n- No hardcoded magic numbers for timeouts remain\n- Tests verify timeout behavior","status":"tombstone","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:56.668782566+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.684684457+01:00","close_reason":"Fixed: Replaced hardcoded 45000ms timeout with DEFAULTS.TIMEOUT_MS constant for consistent timeout behavior across all operations.","labels":["high","quality"]}
{"id":"smartScraper-usb","title":"Fix scrape.test.ts config mock missing getLogLevel","description":"## Problem\n\nThe `scrape.test.ts` test file has an incomplete config mock. The mock only includes `getApiToken` but omits `getLogLevel`, which is required by the logger utility.\n\n## Failing Tests (3)\n- should reject requests without authorization\n- should handle missing request body  \n- should handle malformed JSON\n\n## Root Cause\n`utils/logger.ts:61` calls `getLogLevel()` to determine if a message should be logged. When the config mock doesn't export this function, Vitest throws an error.\n\n## Fix\nUpdate the mock in `src/routes/api/scrape.test.ts`:\n```typescript\nvi.mock('../../config.js', () =\u003e ({\n  getApiToken: () =\u003e 'test-api-token',\n  getLogLevel: () =\u003e 'NONE'  // Add this line\n}));\n```","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:40:55.705738208+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.685971928+01:00","close_reason":"Added getLogLevel to config mock"}
{"id":"smartScraper-wif","title":"Review findings for smartScraper (850226fc): roborev show 14 / one-shot fix with roborev fix 14","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T13:10:49.413167599+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.687244622+01:00"}
{"id":"smartScraper-xn4","title":"Insecure Session Cookie Configuration","description":"Session cookie in auth.ts:67-68 has secure hardcoded to false, violating ADR-014 adaptive security strategy.\n\n## Problem\nLine 68 forces `const isSecure = false;` with comment 'Force secure: false for now'. This exposes session tokens to interception over HTTP, enabling session hijacking attacks on any non-localhost deployment.\n\n## Location\n- src/middleware/auth.ts:64-78\n\n## Risk\nSession hijacking via network interception on non-HTTPS connections. Critical for any production or LAN deployment.\n\n## Action Plan\n1. Read ADR-014 to understand the intended adaptive security behavior\n2. In createSession(), extract hostname from request: `c.req.header('host')?.split(':')[0]`\n3. Define localhost patterns: `['localhost', '127.0.0.1', '0.0.0.0']`\n4. Implement adaptive logic: `const isSecure = getNodeEnv() === 'production' \u0026\u0026 !isLocalhost`\n5. Add debug logging for security decisions\n6. Write test cases for both localhost and production scenarios\n7. Verify cookie behavior in browser DevTools\n\n## Acceptance Criteria\n- Cookies use secure=true in production on non-localhost hosts\n- Cookies use secure=false on localhost for dev convenience\n- Existing tests pass\n- Manual verification in browser confirms correct cookie flags","status":"tombstone","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:34.892304795+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.68855258+01:00","close_reason":"Fixed: Implemented adaptive security per ADR-014. secure=true only in production on non-localhost hosts.","labels":["critical","security"]}
{"id":"smartScraper-y35","title":"Review findings for smartScraper (61d75bff): roborev show 12 / one-shot fix with roborev fix 12","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T13:04:18.578513158+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.689845759+01:00"}
{"id":"smartScraper-y7m","title":"Audit Remediation: Fix identified failure points","description":"Epic tracking all fixes identified in the January 2025 code audit. The audit analyzed potential failure points across the SmartScraper codebase. This epic contains child tasks for each validated issue, organized by severity and category.\n\n## Scope\n- Memory management issues (rate limiting, dead code cleanup)\n- Resource cleanup (browser page leaks)\n- Code quality improvements (logging, encoding)\n- Minor enhancements\n\n## Success Criteria\n- All child tasks completed\n- No regressions (tests pass)\n- Version bump applied\n\n## Out of Scope\n- Issues determined to be 'by design' per ADRs\n- False positives from initial audit\n- Architectural changes (e.g., Redis for rate limiting)\n\n## Reference\nSee AUDIT_RESULT.md for the original findings and critical analysis.","status":"tombstone","priority":1,"issue_type":"epic","owner":"bogorad@gmail","created_at":"2026-02-05T11:25:44.979385797+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.691130961+01:00","close_reason":"Epic complete: All 8 child tasks resolved"}
{"id":"smartScraper-y7m.1","title":"Add TTL cleanup for rate limit store","description":"## Problem\nThe rate limiting middleware at `src/middleware/rate-limit.ts` uses an in-memory Map to track request counts per client. While the keys include a time-window component, old entries are never explicitly removed from the Map.\n\n## Current Behavior\n```typescript\nconst store = new Map\u003cstring, RateLimitEntry\u003e();\n// Keys: `${identifier}:${Math.floor(now / windowMs)}`\n// Old windows accumulate indefinitely\n```\n\n## Impact\n- **Severity**: Low-Medium (initially marked Critical, revised after analysis)\n- **Practical Risk**: Low for authenticated API with limited clients\n- **Memory Growth**: ~100 bytes per unique client per window\n\n## Root Cause\nNo cleanup mechanism exists for expired rate limit windows.\n\n## Solution\nAdd periodic cleanup that runs every minute to remove entries where `resetTime \u003c now`:\n\n```typescript\n// At module level, after store declaration\nsetInterval(() =\u003e {\n  const now = Date.now();\n  for (const [key, entry] of store) {\n    if (entry.resetTime \u003c now) {\n      store.delete(key);\n    }\n  }\n}, 60000);\n```\n\n## Testing\n1. Unit test: Verify entries are cleaned up after window expires\n2. Manual: Monitor memory under load\n\n## Files to Modify\n- `src/middleware/rate-limit.ts`\n\n## Acceptance Criteria\n- [ ] Cleanup interval added\n- [ ] Old entries removed after window expires\n- [ ] No performance regression\n- [ ] Existing tests pass","status":"tombstone","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:02.545137513+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.692535844+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.1","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:02.54840518+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.2","title":"Remove dead code: unused getFileMutex function","description":"## Problem\nThe `getFileMutex` function in `src/utils/mutex.ts` (lines 35-44) is exported but never called anywhere in the codebase. The original audit flagged this as a 'Critical memory leak' but upon investigation, the function is simply dead code.\n\n## Current State\n```typescript\nconst mutexes = new Map\u003cstring, Mutex\u003e();\n\nexport function getFileMutex(path: string): Mutex {\n  let mutex = mutexes.get(path);\n  if (!mutex) {\n    mutex = new Mutex();\n    mutexes.set(path, mutex);\n  }\n  return mutex;\n}\n```\n\n## Evidence of Non-Use\n- `FsKnownSitesAdapter`: Uses `private mutex = new Mutex()` (instance field)\n- `stats-storage.ts`: Uses `const statsMutex = new Mutex()` (module-level)\n- No imports of `getFileMutex` found in codebase\n\n## Impact\n- **Severity**: Low (dead code, not a runtime issue)\n- **Risk**: None (code is never executed)\n- **Benefit**: Cleaner codebase, reduces confusion\n\n## Solution\nRemove the `getFileMutex` function and its supporting `mutexes` Map.\n\n## Files to Modify\n- `src/utils/mutex.ts`: Remove lines 35-44\n\n## Acceptance Criteria\n- [ ] `getFileMutex` function removed\n- [ ] `mutexes` Map removed\n- [ ] TypeScript compiles without errors\n- [ ] No imports break (there are none)","status":"tombstone","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:13.507938338+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.693879502+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.2","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:13.510346325+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.3","title":"Fix potential browser resource leak in loadPage exception path","description":"## Problem\nIn `src/adapters/puppeteer-browser.ts`, the `loadPage` method can leak browser instances if an exception occurs after `puppeteer.launch()` succeeds but before the pageId is returned and stored in `sessions`.\n\n## Current Flow\n```typescript\nasync loadPage(url: string, options?: LoadPageOptions): Promise\u003c{ pageId: string }\u003e {\n  const browser = await puppeteer.launch({ ... });  // 1. Browser created\n  const page = await browser.newPage();              // 2. Can throw here\n  // ... setup operations ...                         // 3. Can throw here\n  const pageId = `page-${++this.pageCounter}`;\n  this.sessions.set(pageId, { page, browser, userDataDir });  // 4. Only here is it tracked\n  return { pageId };\n}\n```\n\n## Failure Scenarios\n1. `browser.newPage()` throws → browser leaked\n2. `page.setUserAgent()` throws → browser + page leaked\n3. `page.goto()` throws → browser + page leaked\n4. Any operation before `sessions.set()` throws → leaked\n\n## Impact\n- **Severity**: Medium\n- **Likelihood**: Low (these operations rarely fail)\n- **Consequence**: Orphaned Chromium processes consuming memory/CPU\n- **Detection**: Hard to detect without process monitoring\n\n## Solution\nWrap the browser creation in try-catch and ensure cleanup:\n\n```typescript\nasync loadPage(url: string, options?: LoadPageOptions): Promise\u003c{ pageId: string }\u003e {\n  const userDataDir = await fs.promises.mkdtemp(...);\n  let browser: Browser | null = null;\n  \n  try {\n    browser = await puppeteer.launch({ ... });\n    const page = await browser.newPage();\n    // ... rest of setup ...\n    \n    const pageId = `page-${++this.pageCounter}`;\n    this.sessions.set(pageId, { page, browser, userDataDir });\n    return { pageId };\n  } catch (error) {\n    // Cleanup on failure\n    if (browser) {\n      await browser.close().catch(() =\u003e {});\n    }\n    await fs.promises.rm(userDataDir, { recursive: true, force: true }).catch(() =\u003e {});\n    throw error;\n  }\n}\n```\n\n## Additional Fix\nAlso convert `fs.mkdtempSync` to `fs.promises.mkdtemp` (async) while we're here.\n\n## Files to Modify\n- `src/adapters/puppeteer-browser.ts`: Lines 45-122\n\n## Testing\n1. Unit test: Mock puppeteer.launch to succeed, newPage to throw\n2. Verify browser.close() is called\n3. Verify userDataDir is cleaned up\n\n## Acceptance Criteria\n- [ ] Browser cleanup on exception\n- [ ] userDataDir cleanup on exception\n- [ ] Sync mkdtempSync converted to async\n- [ ] Existing tests pass\n- [ ] Add test for cleanup path","status":"tombstone","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:32.310141086+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.695325004+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.3","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:32.313857147+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.4","title":"Add error event handler to logger file stream","description":"## Problem\nThe logger in `src/utils/logger.ts` silently fails when file operations encounter errors. If the disk fills up or permissions change, log entries are lost without any notification.\n\n## Current Behavior\n```typescript\n// Line 50-52: Silent catch on init\n} catch (error) {\n  // Silently fail if config not ready yet - will retry on first log\n}\n\n// Line 66-70: Silent catch on write\ntry {\n  logFileStream.write(JSON.stringify(entry) + '\\n');\n} catch (error) {\n  // Fail silently to not disrupt the application\n}\n```\n\n## Impact\n- **Severity**: Low\n- **Consequence**: Disk issues go unnoticed; console output still works\n- **Detection**: Only noticed when reviewing logs and finding gaps\n\n## Solution\nAdd error event handler to the WriteStream and log failures to console:\n\n```typescript\nfunction initLogFile() {\n  if (logFileStream) return;\n  \n  try {\n    const dataDir = getDataDir();\n    const logDir = path.join(dataDir, 'logs');\n    fs.mkdirSync(logDir, { recursive: true });\n    \n    const timestamp = new Date().toISOString().split('T')[0];\n    const logFile = path.join(logDir, `scraper-${timestamp}.jsonl`);\n    \n    logFileStream = fs.createWriteStream(logFile, { flags: 'a' });\n    \n    // Add error handler\n    logFileStream.on('error', (err) =\u003e {\n      console.error('[LOGGER] File stream error:', err.message);\n      logFileStream = null; // Disable file logging, continue with console\n    });\n    \n    // ... rest of function\n  } catch (error) {\n    console.error('[LOGGER] Failed to initialize log file:', \n      error instanceof Error ? error.message : String(error));\n    // Console logging still works\n  }\n}\n```\n\n## Files to Modify\n- `src/utils/logger.ts`: Lines 31-53, 60-72\n\n## Acceptance Criteria\n- [ ] Error event handler added to WriteStream\n- [ ] Init errors logged to console (not silent)\n- [ ] Write errors logged to console (not silent)\n- [ ] File logging gracefully disabled on error\n- [ ] Console logging continues working","status":"tombstone","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:51.609594326+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.696872263+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.4","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:51.614686256+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.5","title":"URL-encode credentials in buildSessionProxyUrl","description":"## Problem\nThe `buildSessionProxyUrl` function in `src/utils/proxy.ts` constructs proxy URLs by string concatenation without URL-encoding the credentials. If the login or password contains special characters (`@`, `:`, `/`, `%`), the resulting URL will be malformed.\n\n## Current Code\n```typescript\n// Line 76\nconst proxyUrl = `http://${sessionLogin}:${password}@${host}`;\n```\n\n## Example Failure\nIf password is `p@ss:word`, the URL becomes:\n```\nhttp://user:p@ss:word@host:port\n         ^  ^ These break URL parsing\n```\n\n## Impact\n- **Severity**: Low\n- **Likelihood**: Low (credentials come from environment variables set by operator)\n- **Consequence**: Proxy authentication fails with special characters\n\n## Note on Input Source\nThe credentials come from `DATADOME_PROXY_*` environment variables, which are operator-controlled (not user input). However, proper encoding is still best practice.\n\n## Solution\nUse `encodeURIComponent` for credentials:\n\n```typescript\nexport function buildSessionProxyUrl(\n  host: string,\n  login: string,\n  password: string,\n  sessionMinutes: number = 2\n): string {\n  const sessionId = crypto.randomUUID().slice(0, 8);\n  const baseLogin = login.split('-session-')[0];\n  const sessionLogin = `${baseLogin}-session-${sessionId}-sessTime-${sessionMinutes}`;\n  \n  // URL-encode credentials to handle special characters\n  const encodedLogin = encodeURIComponent(sessionLogin);\n  const encodedPassword = encodeURIComponent(password);\n  \n  return `http://${encodedLogin}:${encodedPassword}@${host}`;\n}\n```\n\n## Files to Modify\n- `src/utils/proxy.ts`: Lines 60-88\n\n## Testing\n1. Unit test with password containing `@`, `:`, `/`, `%`\n2. Verify URL parses correctly with `new URL()`\n\n## Acceptance Criteria\n- [ ] Credentials URL-encoded\n- [ ] Test added for special characters\n- [ ] Existing proxy tests pass","status":"tombstone","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:09.723331859+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.698236656+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.5","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:09.727924794+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.6","title":"Add queue size limit to CoreScraperEngine","description":"## Problem\nThe PQueue in `src/core/engine.ts` has no maximum size limit. While rate limiting (10 req/min) provides some protection, the queue can still grow if requests arrive faster than they're processed.\n\n## Current Code\n```typescript\n// Line 22\nprivate queue = new PQueue({ concurrency: 5 });\n```\n\n## Impact\n- **Severity**: Low (mitigated by rate limiting)\n- **Likelihood**: Low (requires sustained high traffic beyond rate limit)\n- **Consequence**: Memory growth, eventual OOM\n\n## Analysis\nWith rate limiting at 10 req/min and scrape time ~120s, theoretical queue depth:\n- 10 requests/min × 2 min max = 20 requests in flight\n- Each request is just a Promise reference (~100 bytes)\n- Practical risk is minimal\n\n## Solution\nAdd explicit queue size check with informative error:\n\n```typescript\nexport class CoreScraperEngine {\n  private queue = new PQueue({ concurrency: 5 });\n  private static readonly MAX_QUEUE_SIZE = 100;\n\n  async scrapeUrl(url: string, options?: ScrapeOptions): Promise\u003cScrapeResult\u003e {\n    // Reject if queue is too deep (backpressure)\n    if (this.queue.size \u003e= CoreScraperEngine.MAX_QUEUE_SIZE) {\n      logger.warn('[ENGINE] Queue full, rejecting request', { \n        queueSize: this.queue.size, \n        url \n      });\n      return {\n        success: false,\n        errorType: ERROR_TYPES.CONFIGURATION,\n        error: 'Server overloaded, please retry later'\n      };\n    }\n\n    return await this.queue.add(async () =\u003e {\n      // ... existing code\n    });\n  }\n}\n```\n\n## Configuration\nConsider making MAX_QUEUE_SIZE configurable via environment variable in future.\n\n## Files to Modify\n- `src/core/engine.ts`: Lines 22, 48-74\n\n## Acceptance Criteria\n- [ ] MAX_QUEUE_SIZE constant added\n- [ ] Queue size check before adding\n- [ ] Informative error returned when queue full\n- [ ] Warning logged when queue full\n- [ ] Existing tests pass","status":"tombstone","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:25.229337791+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.699684595+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.6","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:25.232323063+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.7","title":"Validate LLM response Content-Type before parsing","description":"## Problem\nThe `OpenRouterLlmAdapter` in `src/adapters/openrouter-llm.ts` doesn't validate the Content-Type of the API response before accessing JSON fields. If OpenRouter returns an error page (HTML) with status 200, the error handling will be confusing.\n\n## Current Code\n```typescript\n// Lines 62-63\nconst content = response.data?.choices?.[0]?.message?.content;\nif (!content) return [];\n```\n\n## Impact\n- **Severity**: Low\n- **Likelihood**: Very low (APIs don't typically return HTML with 200)\n- **Consequence**: Confusing empty result instead of clear error\n\n## Note\nThe current code already handles this gracefully - if `choices` is undefined (as it would be for HTML), it returns `[]`. The fix is primarily for better error messages.\n\n## Solution\nAdd Content-Type validation and better logging:\n\n```typescript\ntry {\n  const response = await axios.post(...);\n  \n  // Validate response type\n  const contentType = response.headers['content-type'] || '';\n  if (!contentType.includes('application/json')) {\n    logger.warn('[LLM] Unexpected response type', { \n      contentType,\n      status: response.status \n    });\n    return [];\n  }\n  \n  const content = response.data?.choices?.[0]?.message?.content;\n  if (!content) {\n    logger.debug('[LLM] Empty content in response', { \n      hasChoices: !!response.data?.choices \n    });\n    return [];\n  }\n  \n  // ... rest of parsing\n} catch (error) {\n  // ... existing error handling\n}\n```\n\n## Files to Modify\n- `src/adapters/openrouter-llm.ts`: Lines 40-78\n\n## Acceptance Criteria\n- [ ] Content-Type validated\n- [ ] Warning logged for unexpected types\n- [ ] Debug logging for empty responses\n- [ ] Existing behavior preserved (return [] on failure)","status":"tombstone","priority":4,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:42.514385538+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.701393755+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.7","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:42.518831805+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.9","title":"Update AUDIT_RESULT.md with critical analysis findings","description":"## Task\nUpdate the AUDIT_RESULT.md file with the critical analysis that distinguished valid issues from false positives.\n\n## Required Updates\n1. Add 'Critical Analysis' section showing validity assessment for each finding\n2. Mark issues that were false positives with clear explanation\n3. Mark issues that are 'by design' per ADRs with references\n4. Update severity levels based on analysis\n5. Add summary table of valid vs invalid findings\n\n## Key Findings from Analysis\n| # | Issue | Original | Revised | Reason |\n|---|-------|----------|---------|--------|\n| 2 | Mutex registry leak | Critical | N/A | Dead code - getFileMutex never called |\n| 3 | Process exit | High | By design | Documented in ADR-011 |\n| 6 | Stats race condition | Medium | N/A | Mutex already protects correctly |\n| 7 | XPath validation bypass | Medium | N/A | Browser sandbox prevents exploitation |\n| 11 | Session cookie httpOnly | Medium | N/A | Misread - httpOnly is always true |\n\n## Valid Issues (to be fixed)\n- #1 Rate limit store (Low-Medium)\n- #5 Sync file operations (Low) - bundled with #13\n- #8 CAPTCHA abort (Low)\n- #9 Logger error handling (Low)\n- #10 Proxy URL encoding (Low)\n- #12 Queue size limit (Low)\n- #13 Browser resource leak (Medium)\n- #14 LLM Content-Type (Low)\n\n## Files to Modify\n- AUDIT_RESULT.md\n\n## Acceptance Criteria\n- [ ] Each of 15 findings has validity assessment\n- [ ] False positives have clear explanations\n- [ ] ADR references for 'by design' items\n- [ ] Summary table shows valid count vs total","status":"tombstone","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-02-05T11:28:27.815542037+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T14:11:56.70291335+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:28:27.81878659+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.1","type":"blocks","created_at":"2026-02-05T11:28:36.530871516+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.2","type":"blocks","created_at":"2026-02-05T11:28:36.603524621+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.3","type":"blocks","created_at":"2026-02-05T11:28:36.676114718+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.4","type":"blocks","created_at":"2026-02-05T11:28:36.750211864+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.5","type":"blocks","created_at":"2026-02-05T11:28:36.8223172+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.6","type":"blocks","created_at":"2026-02-05T11:28:36.896562827+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.7","type":"blocks","created_at":"2026-02-05T11:28:36.96678639+01:00","created_by":"Eugene Bogorad"}]}
