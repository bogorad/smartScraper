{"id":"smartScraper-0vk","title":"Missing Input Validation on Dashboard Parameters","description":"Dashboard routes accept query params without validation.\n\n## Problem\nQuery parameters (page, limit, sort) at sites.tsx:20-23 are used directly without Zod validation. parseInt on invalid input returns NaN, potentially causing unexpected behavior or errors downstream.\n\n## Location\n- src/routes/dashboard/sites.tsx:20-23\n\n## Risk\nMalformed query parameters could cause application errors or unexpected behavior. Low security risk but poor code quality.\n\n## Action Plan\n1. Create Zod schema for dashboard query parameters:\n   ```typescript\n   const querySchema = z.object({\n     q: z.string().optional().default(''),\n     sort: z.enum(['domain', 'failures', 'last']).optional().default('domain'),\n     limit: z.union([z.literal('all'), z.coerce.number().int().min(1).max(100)]).optional().default(10),\n     page: z.coerce.number().int().min(1).optional().default(1)\n   });\n   ```\n2. Apply zValidator middleware or manual parse at route handler start\n3. Use parsed/validated values instead of raw query strings\n4. Return 400 Bad Request for invalid parameters with helpful message\n5. Add tests for valid params, invalid params, and edge cases\n\n## Acceptance Criteria\n- All query parameters are validated before use\n- Invalid parameters return 400 with descriptive error\n- Default values are applied for missing optional params\n- parseInt is never called on unvalidated strings\n- Tests cover validation scenarios","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:05.189012091+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:47:09.70847125+01:00","closed_at":"2026-01-31T13:47:09.70847125+01:00","close_reason":"Fixed: Added Zod validation schema for dashboard query parameters (q, sort, limit, page) with proper types and constraints.","labels":["medium","security"]}
{"id":"smartScraper-1q4","title":"No Rate Limiting on API Endpoints","description":"API endpoints have no rate limiting, allowing unlimited requests from authenticated clients.\n\n## Problem\nEnables:\n1. DoS via resource exhaustion (browser instances, memory, CPU)\n2. Cost abuse on external services (2Captcha charges per solve, OpenRouter charges per token)\n3. Target website abuse leading to IP bans and potential legal liability\n\n## Location\n- src/routes/api/scrape.ts\n- src/routes/dashboard/*.tsx\n\n## Risk\nResource exhaustion, runaway costs, legal issues from aggressive scraping. Any authenticated user can trigger unlimited scrapes.\n\n## Action Plan\n1. Create src/middleware/rate-limit.ts with rateLimitMiddleware factory\n2. Use in-memory store with sliding window: Map\u003cstring, {count, resetTime}\u003e\n3. Identify clients by Authorization header or x-forwarded-for IP\n4. Configure different limits per endpoint:\n   - /api/scrape: 10 requests/minute (scraping is expensive)\n   - /dashboard/*: 60 requests/minute (UI interactions)\n5. Return 429 Too Many Requests with Retry-After header\n6. Add rate limit headers to responses (X-RateLimit-Remaining, X-RateLimit-Reset)\n7. Apply middleware to scrapeRouter and dashboardRouter\n8. Add configuration options via config.ts for limit customization\n9. Write tests for rate limit enforcement and reset behavior\n\n## Acceptance Criteria\n- Requests exceeding limit receive 429 response\n- Rate limit headers present on all responses\n- Limits are configurable via environment variables\n- Legitimate usage patterns are not impacted\n- Tests verify limit enforcement and window reset","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:41.803170249+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:41:52.701476912+01:00","closed_at":"2026-01-31T13:41:52.701476912+01:00","close_reason":"Fixed: Implemented rate limiting middleware with 10 req/min for API and 60 req/min for dashboard. Includes rate limit headers and 429 responses.","labels":["critical","security"]}
{"id":"smartScraper-2gg","title":"2Captcha Polling Continues After Fatal Errors","description":"solveDataDome does not detect all fatal 2Captcha API errors.\n\n## Problem\nError detection at lines 136-138 checks for status=error OR non-zero errorId, but some fatal conditions may have status=processing with an errorCode field. This causes wasted polling iterations and delayed failure responses.\n\n## Location\n- src/adapters/twocaptcha.ts:116-139\n\n## Risk\nWasted API calls (cost), delayed user feedback, potential timeout before error is properly reported.\n\n## Action Plan\n1. Research 2Captcha API documentation for all possible error response formats\n2. Identify fatal error codes that should terminate polling immediately:\n   - ERROR_CAPTCHA_UNSOLVABLE\n   - ERROR_WRONG_CAPTCHA_ID\n   - ERROR_BAD_TOKEN_OR_PAGEURL\n   - etc.\n3. Update polling loop condition to check:\n   - status === 'error'\n   - errorId !== 0\n   - presence of errorCode field (regardless of status)\n4. Extract error checking into helper function: isFatalError(response): boolean\n5. Add specific error messages for known fatal codes\n6. Add test cases with mocked 2Captcha responses for each error type\n\n## Acceptance Criteria\n- All fatal 2Captcha errors terminate polling immediately\n- Specific error messages returned for known error codes\n- No unnecessary polling iterations after fatal error\n- Tests cover all documented error response formats","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:14.749452616+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:49:31.121989655+01:00","closed_at":"2026-01-31T13:49:31.121989655+01:00","close_reason":"Fixed: Enhanced error detection to check for errorCode field regardless of status. Added mapping for known fatal error codes to provide better error messages and terminate polling immediately.","labels":["medium","quality"]}
{"id":"smartScraper-30m","title":"Review findings for smartScraper (208cff3d): roborev show 2 / one-shot fix with roborev fix 2","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:38:54.59864411+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:49:28.667726629+01:00","closed_at":"2026-02-05T11:49:28.667726629+01:00","close_reason":"Fixed typo: 'to that' -\u003e 'so that' in AGENTS.md workflow pattern"}
{"id":"smartScraper-5kf","title":"No CSRF Protection on Dashboard Forms","description":"Dashboard POST endpoints lack CSRF protection tokens.\n\n## Problem\nPOST endpoints like /dashboard/sites/:domain do not implement CSRF tokens. HTMX requests from authenticated sessions are vulnerable to cross-site request forgery attacks.\n\n## Location\n- src/routes/dashboard/sites.tsx:269-360\n\n## Risk\nAttackers could trick authenticated users into performing unwanted actions (deleting sites, modifying configurations) via malicious links or embedded forms on third-party sites.\n\n## Action Plan\n1. Create src/middleware/csrf.ts with csrfMiddleware\n2. On GET requests: generate crypto.randomUUID() token, store in cookie (httpOnly: false for JS access)\n3. Set token in Hono context: c.set('csrfToken', token)\n4. On POST/PUT/DELETE: validate X-CSRF-Token header matches cookie\n5. Return 403 Forbidden if validation fails\n6. Update layout.tsx to include CSRF token in a meta tag\n7. Configure HTMX to send X-CSRF-Token header on all requests via hx-headers\n8. Apply csrfMiddleware to dashboardRouter\n9. Write tests for token generation, validation success, and validation failure\n\n## Acceptance Criteria\n- All dashboard POST requests require valid CSRF token\n- Missing or invalid token returns 403 response\n- HTMX requests automatically include token via configured headers\n- Token regenerates on each page load\n- Tests cover happy path and attack scenarios","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:49.772971002+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:43:38.021221655+01:00","closed_at":"2026-01-31T13:43:38.021221655+01:00","close_reason":"Fixed: Implemented CSRF middleware with token generation on GET and validation on POST/PUT/DELETE. HTMX configured to send X-CSRF-Token header automatically.","labels":["high","security"]}
{"id":"smartScraper-6ax","title":"Missing Error Handling in Async Operations","description":"Empty catch blocks swallow errors silently.\n\n## Problem\nSeveral async operations use empty catch blocks, hiding critical errors from logs and monitoring. This makes debugging difficult and can mask underlying issues.\n\n## Location\n- src/adapters/puppeteer-browser.ts:33-35 (closePage cleanup)\n- src/core/engine.ts:296-300 (finally block cleanup)\n\n## Risk\nErrors go undetected, making debugging difficult. Potential resource leaks if cleanup fails silently. Production issues may be impossible to diagnose.\n\n## Action Plan\n1. Identify all empty catch blocks in codebase: `rg 'catch.*\\{\\s*\\}' src/`\n2. For each empty catch, determine appropriate handling:\n   - Expected failures (e.g., closing already-closed page): log at DEBUG level\n   - Unexpected failures: log at WARN level with error details\n3. Update puppeteer-browser.ts:33-35:\n   ```typescript\n   try { await session.page.close(); } \n   catch (e) { logger.debug('Page close failed (may already be closed)', { error: String(e) }); }\n   ```\n4. Update engine.ts:296-300 similarly\n5. Review other catch blocks for completeness\n6. Add eslint rule to prevent empty catch blocks: no-empty\n\n## Acceptance Criteria\n- No empty catch blocks remain in codebase\n- All caught errors are logged with appropriate level\n- Expected failures logged at DEBUG (not noisy)\n- Unexpected failures logged at WARN (visible)\n- ESLint configured to prevent future empty catches","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:17.187347472+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:50:18.474446152+01:00","closed_at":"2026-01-31T13:50:18.474446152+01:00","close_reason":"Fixed: Added proper error logging to empty catch blocks in puppeteer-browser.ts and engine.ts. Errors are logged at DEBUG level since they represent expected cleanup failures.","labels":["medium","quality"]}
{"id":"smartScraper-92n","title":"Information Disclosure in Error Messages","description":"Error responses expose internal implementation details.\n\n## Problem\nSeveral error responses leak stack traces, file paths, and internal state that could aid attackers in reconnaissance. Examples include raw error.message from exceptions and detailed 2Captcha API responses.\n\n## Location\n- src/routes/api/scrape.ts\n- src/adapters/twocaptcha.ts\n- src/core/engine.ts\n\n## Risk\nInformation leakage aids attackers in understanding system architecture, identifying vulnerable components, and crafting targeted attacks.\n\n## Action Plan\n1. Create src/utils/error-sanitizer.ts with sanitizeErrorForClient(error): string function\n2. Define allowlist of safe error messages to expose (e.g., 'Invalid URL', 'Rate limit exceeded')\n3. Map internal errors to generic client-facing messages\n4. Update scrape.ts error handler to use sanitized messages in JSON response\n5. Keep detailed error logging internal via logger.error()\n6. Review twocaptcha.ts responses - strip API-specific details\n7. Review engine.ts - ensure stack traces never reach client\n8. Add error response tests verifying no internal details leak\n\n## Acceptance Criteria\n- Client-facing error responses contain only generic, safe messages\n- Stack traces never appear in API responses\n- Internal file paths never appear in API responses\n- Detailed errors are logged server-side for debugging\n- Tests verify error sanitization","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:59.321813794+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:46:09.350958192+01:00","closed_at":"2026-01-31T13:46:09.350958192+01:00","close_reason":"Fixed: Created error-sanitizer.ts with sanitizeErrorForClient() function. Updated scrape endpoint to catch errors and return sanitized messages to clients while logging full details internally.","labels":["medium","security"]}
{"id":"smartScraper-9pg","title":"Review findings for smartScraper (68e8e864): roborev show 3 / one-shot fix with roborev fix 3","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:49:02.870212754+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:50:17.977150578+01:00","closed_at":"2026-02-05T11:50:17.977150578+01:00","close_reason":"Reviewed: 5 low/medium improvement suggestions. Type safety, test coverage gaps, and CHANGELOG update are valid points for future enhancement. No blocking issues."}
{"id":"smartScraper-ahl","title":"Potential ReDoS in DOM Simplification","description":"simplifyDom function uses regex patterns vulnerable to catastrophic backtracking.\n\n## Problem\nRegex patterns like `\u003ctag[^\u003e]*\u003e.*?\u003c/tag\u003e` with gis flags on untrusted HTML can cause exponential backtracking with crafted nested structures. The .*? quantifier combined with alternation in complex HTML can trigger ReDoS.\n\n## Location\n- src/utils/dom.ts:8-38\n\n## Risk\nCPU exhaustion via crafted HTML input. Severity is Medium because input comes from scraped pages (not direct user input), but a malicious target site could exploit this.\n\n## Action Plan\n1. Add MAX_HTML_SIZE constant (1MB) at top of file\n2. Truncate input at start of simplifyDom: `if (html.length \u003e MAX_HTML_SIZE) html = html.slice(0, MAX_HTML_SIZE)`\n3. Consider replacing regex approach with DOM-based parsing:\n   - Use linkedom or happy-dom to parse HTML\n   - Use querySelectorAll to find and remove unwanted tags\n   - Use DOM API for class-based removal\n4. If keeping regex, add execution timeout wrapper\n5. Benchmark both approaches for performance comparison\n6. Add test cases with large HTML inputs and nested structures\n\n## Acceptance Criteria\n- HTML inputs over 1MB are truncated before processing\n- Processing completes in bounded time regardless of input structure\n- Existing functionality preserved for normal inputs\n- Performance benchmarks show acceptable overhead\n- No regression in LLM prompt generation quality","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:53.417945548+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:44:23.414944267+01:00","closed_at":"2026-01-31T13:44:23.414944267+01:00","close_reason":"Fixed: Added MAX_HTML_SIZE constant (1MB) to truncate oversized inputs before regex processing, preventing ReDoS attacks via crafted HTML.","labels":["medium","security"]}
{"id":"smartScraper-bh9","title":"console.log Used Instead of Logger","description":"twocaptcha.ts uses console.log directly instead of centralized logger.\n\n## Problem\nLines 102, 109, and 126 use console.log for debug output instead of the project's logger utility. This violates src/AGENTS.md convention and bypasses log level filtering, structured logging, and any log aggregation setup.\n\n## Location\n- src/adapters/twocaptcha.ts:102, 109, 126\n\n## Risk\nLow severity but poor code hygiene. Debug output may appear in production. Inconsistent log format makes parsing difficult.\n\n## Action Plan\n1. Import logger at top of file: `import { logger } from '../utils/logger.js';`\n2. Replace line 102 console.log with logger.debug\n3. Replace line 109 console.log with logger.debug\n4. Replace line 126 console.log with logger.debug\n5. Use structured logging format: `logger.debug('message', { data }, 'CAPTCHA')`\n6. Ensure sensitive data (API keys) remain redacted\n7. Run grep to verify no console.log remains: `rg 'console\\.log' src/`\n\n## Acceptance Criteria\n- No console.log calls in twocaptcha.ts\n- All logging uses project logger utility\n- Log level is DEBUG (not INFO) for verbose API interaction logs\n- API keys remain redacted in logs\n- Grep confirms no console.log in src/","status":"closed","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:20.349170684+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:51:31.824106838+01:00","closed_at":"2026-01-31T13:51:31.824106838+01:00","close_reason":"Fixed: Replaced all console.log calls with logger.debug() in twocaptcha.ts. API keys remain properly redacted in log output.","labels":["low","quality"]}
{"id":"smartScraper-c51","title":"Fix twocaptcha.test.ts config mock missing getLogLevel","description":"## Problem\n\nThe `twocaptcha.test.ts` test file has an incomplete config mock. The mock includes CAPTCHA-specific config getters but omits `getLogLevel`, which is required by the logger utility used in the TwoCaptchaAdapter.\n\n## Failing Tests (8)\n- should delegate to solveDataDome for DataDome CAPTCHA\n- should successfully solve DataDome CAPTCHA\n- should include proxy details when provided\n- should handle task creation failure\n- should poll until task is ready\n- should handle error status from task result\n- should handle missing cookie in solution\n- should timeout when DataDome solving takes too long\n\n## Root Cause\nThe TwoCaptchaAdapter uses `logger.debug()` at lines 103, 113, 130. The logger calls `getLogLevel()` which is not in the mock.\n\n## Fix\nUpdate the mock in `src/adapters/twocaptcha.test.ts`:\n```typescript\nvi.mock('../config.js', () =\u003e ({\n  getTwocaptchaApiKey: () =\u003e 'test-api-key',\n  getCaptchaDefaultTimeout: () =\u003e 120,\n  getCaptchaPollingInterval: () =\u003e 100,\n  getLogLevel: () =\u003e 'NONE'  // Add this line\n}));\n```","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:14.993560428+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:45:57.232908327+01:00","closed_at":"2026-02-05T11:45:57.232908327+01:00","close_reason":"Added getLogLevel to config mock"}
{"id":"smartScraper-cnv","title":"Potential Memory Leak in SSE Connections","description":"SSE implementation lacks connection limits and stale cleanup.\n\n## Problem\nThe clients Set at index.tsx:44 has no maximum connection limit. No timeout mechanism exists for stale connections that fail to disconnect properly, potentially leading to memory exhaustion.\n\n## Location\n- src/routes/dashboard/index.tsx:39-163\n\n## Risk\nMemory leak if connections accumulate. Potential DoS vector if attacker opens many SSE connections.\n\n## Action Plan\n1. Add MAX_SSE_CLIENTS constant (e.g., 100)\n2. Check clients.size before adding new client in /events handler\n3. Return 503 Service Unavailable if limit exceeded\n4. Add connection timestamp to Client interface\n5. Implement periodic cleanup (every 60s) to remove connections older than 10 minutes\n6. Add heartbeat response tracking - remove clients that fail to receive heartbeat\n7. Log connection/disconnection events for monitoring\n8. Add metrics endpoint or logging for current client count\n\n## Acceptance Criteria\n- New connections rejected with 503 when limit reached\n- Stale connections cleaned up automatically\n- Memory usage bounded regardless of connection patterns\n- Monitoring visibility into connection count\n- Tests verify limit enforcement and cleanup","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:08.382732445+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:47:57.78626026+01:00","closed_at":"2026-01-31T13:47:57.78626026+01:00","close_reason":"Fixed: Added MAX_SSE_CLIENTS limit (100), connection timeout (10 min), and periodic cleanup of stale connections to prevent memory leaks.","labels":["medium","quality"]}
{"id":"smartScraper-g2v","title":"Insufficient Logging of Security Events","description":"Security events logged at inappropriate levels.\n\n## Problem\nFailed authentication attempts and configuration changes are logged at INFO/DEBUG level in auth.ts. Production log configurations often filter these levels, making security incident investigation difficult.\n\n## Location\n- src/middleware/auth.ts\n\n## Risk\nSecurity incidents may go unnoticed. Audit trail incomplete for compliance requirements. Difficult to detect brute-force attacks or unauthorized access attempts.\n\n## Action Plan\n1. Review all logger calls in auth.ts and identify security-relevant events\n2. Create security event categories:\n   - AUTH_FAILURE: Failed login attempts (WARN level)\n   - AUTH_SUCCESS: Successful logins (INFO level, but always captured)\n   - SESSION_CREATED: New session establishment (INFO level)\n   - SESSION_INVALID: Invalid session presented (WARN level)\n3. Update log levels:\n   - logger.warn for all failure cases (currently some use logger.info)\n   - Include relevant context: IP address, user agent, timestamp\n4. Consider structured logging format for security events\n5. Document security logging in ADR or CONFIGURATION.md\n6. Verify production log config captures WARN and above\n\n## Acceptance Criteria\n- All authentication failures logged at WARN or higher\n- Security events include sufficient context for investigation\n- Production log configuration captures all security events\n- Log format suitable for SIEM ingestion if needed","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:31:11.072524028+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:48:47.7250999+01:00","closed_at":"2026-01-31T13:48:47.7250999+01:00","close_reason":"Fixed: Enhanced security event logging in auth.ts with WARN level for authentication failures including IP address and user agent context for audit trail.","labels":["medium","security"]}
{"id":"smartScraper-goi","title":"Critical: Hardcoded timeout in browser reload after CAPTCHA solving","description":"## Problem\n\nKimi Audit Issue #2 (Critical) was NOT addressed in the security fixes.\n\nThe browser reload operation after CAPTCHA solving uses a hardcoded timeout value (Puppeteer's default of 30,000 ms) instead of respecting the user-provided `timeoutMs` parameter.\n\nWhen a CAPTCHA is detected and solved, and cookies are updated, the engine calls `browser.reload(pageId)` without passing any timeout parameter. This causes operations to fail with \"Navigation timeout of 30000 ms exceeded\" even when users explicitly provide custom timeout values.\n\n## Locations\n\n- `src/core/engine.ts` line 157 - calls `reload(pageId)` without timeout\n- `src/ports/browser.ts` line 20 - interface lacks timeout option\n- `src/adapters/puppeteer-browser.ts` lines 278-282 - implementation uses Puppeteer default\n\n## Risk\n\nCAPTCHA-protected sites become inaccessible regardless of user timeout configuration, breaking core functionality.\n\n## Action Plan\n\n1. Update `BrowserPort` interface to accept optional timeout/waitUntil in `reload()`\n2. Update `PuppeteerBrowserAdapter.reload()` to use `options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Update `CoreScraperEngine` to pass `timeoutMs` from scrape options to reload call\n4. Add test coverage for timeout propagation\n\n## Fix Reference (from Kimi audit)\n\n\\`\\`\\`typescript\n// Step 1: Update BrowserPort interface\nreload(pageId: string, options?: { timeout?: number; waitUntil?: 'load' | 'domcontentloaded' | 'networkidle0' | 'networkidle2' }): Promise\u003cvoid\u003e;\n\n// Step 2: Update PuppeteerBrowserAdapter\nasync reload(pageId: string, options?: { timeout?: number; waitUntil?: string }): Promise\u003cvoid\u003e {\n  const session = this.sessions.get(pageId);\n  if (!session) return;\n  await session.page.reload({\n    waitUntil: options?.waitUntil || 'networkidle2',\n    timeout: options?.timeout || DEFAULTS.TIMEOUT_MS\n  });\n}\n\n// Step 3: Update CoreScraperEngine call\nif (solveResult.updatedCookie) {\n  await this.browserPort.setCookies(pageId, solveResult.updatedCookie);\n  await this.browserPort.reload(pageId, {\n    timeout: options?.timeoutMs || DEFAULTS.TIMEOUT_MS,\n    waitUntil: 'networkidle2'\n  });\n}\n\\`\\`\\`","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:55:39.843504235+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:56:38.31367606+01:00","closed_at":"2026-01-31T13:56:38.31367606+01:00","close_reason":"Fixed: reload() now uses DEFAULTS.TIMEOUT_MS (120s) as fallback instead of Puppeteer default (30s) when timeoutMs is undefined. The fix was partially in place; this commit ensures consistent timeout behavior."}
{"id":"smartScraper-gzs","title":"Fix auth.test.ts createSession mock context","description":"## Problem\n\nThe `auth.test.ts` test for `createSession` uses an incomplete mock Hono context. The mock only has a `header` function and `req.url`, but `createSession` requires a full Hono Context with cookie handling methods.\n\n## Failing Test (1)\n- should set session cookie\n\n## Root Cause\nThe test creates a minimal mock:\n```typescript\nconst mockContext = {\n  header: vi.fn(),\n  req: { url: 'http://localhost' }\n};\n```\n\nBut `createSession` calls Hono's `setCookie()` helper which requires proper Context implementation including the response object.\n\n## Fix\nEither:\n1. Use a real Hono test app with `app.request()` pattern (consistent with other tests in the file)\n2. Mock the full Context interface including `res` object and `set` method\n\nRecommended approach - use real Hono app:\n```typescript\nit('should set session cookie', async () =\u003e {\n  const testApp = new Hono();\n  testApp.post('/login', (c) =\u003e {\n    createSession(c, 'test-token');\n    return c.text('OK');\n  });\n\n  const req = new Request('http://localhost/login', { method: 'POST' });\n  const res = await testApp.request(req);\n  \n  expect(res.headers.get('Set-Cookie')).toContain('ss_session=');\n});\n```","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:27.344724451+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:46:24.973485521+01:00","closed_at":"2026-02-05T11:46:24.973485521+01:00","close_reason":"Fixed createSession test to use real Hono app pattern"}
{"id":"smartScraper-nta","title":"Fix engine.test.ts reload signature mismatch","description":"## Problem\n\nThe `engine.test.ts` test for CAPTCHA handling expects `browserPort.reload(pageId)` to be called with only one argument, but the actual implementation passes two arguments.\n\n## Failing Test (1)\n- should handle CAPTCHA detection and solving\n\n## Root Cause\nThe test expectation at line 131:\n```typescript\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123');\n```\n\nBut the actual call in `engine.ts:170`:\n```typescript\nawait this.browserPort.reload(pageId, options?.timeoutMs || DEFAULTS.TIMEOUT_MS);\n```\n\nThe reload method signature includes an optional timeout parameter that was added after the test was written.\n\n## Fix\nUpdate the test expectation in `src/core/engine.test.ts`:\n```typescript\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123', expect.any(Number));\n// Or be more specific:\nexpect(mockBrowser.reload).toHaveBeenCalledWith('page-123', 120000);\n```","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:41:40.518404995+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:46:08.749747473+01:00","closed_at":"2026-02-05T11:46:08.749747473+01:00","close_reason":"Fixed reload expectation to include timeout parameter"}
{"id":"smartScraper-nw5","title":"XPath Injection Vulnerability","description":"evaluateXPath function passes user-controlled XPath expressions directly to page.evaluate() without validation.\n\n## Problem\nNo validation or sanitization of XPath input allows:\n1. CPU exhaustion via computationally expensive queries (e.g., deeply nested predicates)\n2. Memory exhaustion via queries returning massive result sets\n3. Potential information disclosure via reading unexpected document nodes\n\n## Location\n- src/adapters/puppeteer-browser.ts:152-188\n\n## Risk\nResource exhaustion attacks from authenticated users. Requires valid API token, so severity is High rather than Critical.\n\n## Action Plan\n1. Define allowed XPath character pattern: `/^[\\w\\-\\/\\[\\]@=\"'\\s\\.\\(\\)\\|\\*\\:]+$/`\n2. Set maximum XPath length constant: `MAX_XPATH_LENGTH = 500`\n3. Create validateXPath(xpath: string): boolean function\n4. Add validation call at start of evaluateXPath method\n5. Return null and log warning for invalid XPaths\n6. Add timeout protection inside page.evaluate to limit execution time\n7. Write test cases for valid XPaths, malformed XPaths, and oversized XPaths\n\n## Acceptance Criteria\n- Invalid XPaths are rejected before reaching Puppeteer\n- Oversized XPaths (\u003e500 chars) are rejected\n- Valid XPaths continue to work correctly\n- Rejection is logged with truncated XPath for debugging\n- All existing tests pass","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:38.726979396+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:39:13.136582379+01:00","closed_at":"2026-01-31T13:39:13.136582379+01:00","close_reason":"Fixed: Added XPath validation with max length (500 chars) and allowed character pattern to prevent injection attacks.","labels":["high","security"]}
{"id":"smartScraper-t4i","title":"Review findings for smartScraper (6fd9f612): roborev show 1 / one-shot fix with roborev fix 1","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:38:44.941918897+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:49:42.385161407+01:00","closed_at":"2026-02-05T11:49:42.385161407+01:00","close_reason":"Reviewed: 6 low/medium findings, mostly improvement suggestions. Rate-limit race condition not applicable (single-threaded JS). Test coverage and error type improvements can be tracked separately."}
{"id":"smartScraper-ubh","title":"Inconsistent Default Timeout Values","description":"PuppeteerBrowserAdapter uses inconsistent timeout defaults across operations.\n\n## Problem\n- Browser launch uses DEFAULTS.TIMEOUT_MS (120000ms) at line 77\n- Page navigation uses hardcoded 45000ms at line 105\nThis violates DRY principle and causes non-deterministic timeout behavior. Users setting custom timeouts may be confused when navigation uses different defaults than other operations.\n\n## Location\n- src/adapters/puppeteer-browser.ts:77, 105\n\n## Risk\nFunctional bugs where navigation times out unexpectedly, or operations take longer than expected. Violates principle of least surprise.\n\n## Action Plan\n1. Locate all hardcoded timeout values in puppeteer-browser.ts\n2. Replace line 105 timeout: `timeout: options?.timeout || DEFAULTS.TIMEOUT_MS`\n3. Search for other hardcoded timeouts in the file (reload, waitForSelector, etc.)\n4. Ensure all timeout parameters consistently use DEFAULTS.TIMEOUT_MS as fallback\n5. Consider adding DEFAULTS.NAVIGATION_TIMEOUT if navigation needs different default\n6. Update any related documentation in ADR-001\n7. Add test to verify timeout propagation from options to Puppeteer calls\n\n## Acceptance Criteria\n- All timeout parameters use DEFAULTS.TIMEOUT_MS consistently\n- Custom timeout options are respected throughout the call chain\n- No hardcoded magic numbers for timeouts remain\n- Tests verify timeout behavior","status":"closed","priority":1,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:56.668782566+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:44:57.752733199+01:00","closed_at":"2026-01-31T13:44:57.752733199+01:00","close_reason":"Fixed: Replaced hardcoded 45000ms timeout with DEFAULTS.TIMEOUT_MS constant for consistent timeout behavior across all operations.","labels":["high","quality"]}
{"id":"smartScraper-usb","title":"Fix scrape.test.ts config mock missing getLogLevel","description":"## Problem\n\nThe `scrape.test.ts` test file has an incomplete config mock. The mock only includes `getApiToken` but omits `getLogLevel`, which is required by the logger utility.\n\n## Failing Tests (3)\n- should reject requests without authorization\n- should handle missing request body  \n- should handle malformed JSON\n\n## Root Cause\n`utils/logger.ts:61` calls `getLogLevel()` to determine if a message should be logged. When the config mock doesn't export this function, Vitest throws an error.\n\n## Fix\nUpdate the mock in `src/routes/api/scrape.test.ts`:\n```typescript\nvi.mock('../../config.js', () =\u003e ({\n  getApiToken: () =\u003e 'test-api-token',\n  getLogLevel: () =\u003e 'NONE'  // Add this line\n}));\n```","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:40:55.705738208+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:45:55.355581314+01:00","closed_at":"2026-02-05T11:45:55.355581314+01:00","close_reason":"Added getLogLevel to config mock"}
{"id":"smartScraper-xn4","title":"Insecure Session Cookie Configuration","description":"Session cookie in auth.ts:67-68 has secure hardcoded to false, violating ADR-014 adaptive security strategy.\n\n## Problem\nLine 68 forces `const isSecure = false;` with comment 'Force secure: false for now'. This exposes session tokens to interception over HTTP, enabling session hijacking attacks on any non-localhost deployment.\n\n## Location\n- src/middleware/auth.ts:64-78\n\n## Risk\nSession hijacking via network interception on non-HTTPS connections. Critical for any production or LAN deployment.\n\n## Action Plan\n1. Read ADR-014 to understand the intended adaptive security behavior\n2. In createSession(), extract hostname from request: `c.req.header('host')?.split(':')[0]`\n3. Define localhost patterns: `['localhost', '127.0.0.1', '0.0.0.0']`\n4. Implement adaptive logic: `const isSecure = getNodeEnv() === 'production' \u0026\u0026 !isLocalhost`\n5. Add debug logging for security decisions\n6. Write test cases for both localhost and production scenarios\n7. Verify cookie behavior in browser DevTools\n\n## Acceptance Criteria\n- Cookies use secure=true in production on non-localhost hosts\n- Cookies use secure=false on localhost for dev convenience\n- Existing tests pass\n- Manual verification in browser confirms correct cookie flags","status":"closed","priority":0,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-01-31T13:30:34.892304795+01:00","created_by":"Eugene Bogorad","updated_at":"2026-01-31T13:38:36.109465233+01:00","closed_at":"2026-01-31T13:38:36.109465233+01:00","close_reason":"Fixed: Implemented adaptive security per ADR-014. secure=true only in production on non-localhost hosts.","labels":["critical","security"]}
{"id":"smartScraper-y7m","title":"Audit Remediation: Fix identified failure points","description":"Epic tracking all fixes identified in the January 2025 code audit. The audit analyzed potential failure points across the SmartScraper codebase. This epic contains child tasks for each validated issue, organized by severity and category.\n\n## Scope\n- Memory management issues (rate limiting, dead code cleanup)\n- Resource cleanup (browser page leaks)\n- Code quality improvements (logging, encoding)\n- Minor enhancements\n\n## Success Criteria\n- All child tasks completed\n- No regressions (tests pass)\n- Version bump applied\n\n## Out of Scope\n- Issues determined to be 'by design' per ADRs\n- False positives from initial audit\n- Architectural changes (e.g., Redis for rate limiting)\n\n## Reference\nSee AUDIT_RESULT.md for the original findings and critical analysis.","status":"closed","priority":1,"issue_type":"epic","owner":"bogorad@gmail","created_at":"2026-02-05T11:25:44.979385797+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:33.44609167+01:00","closed_at":"2026-02-05T11:37:33.44609167+01:00","close_reason":"Epic complete: All 8 child tasks resolved"}
{"id":"smartScraper-y7m.1","title":"Add TTL cleanup for rate limit store","description":"## Problem\nThe rate limiting middleware at `src/middleware/rate-limit.ts` uses an in-memory Map to track request counts per client. While the keys include a time-window component, old entries are never explicitly removed from the Map.\n\n## Current Behavior\n```typescript\nconst store = new Map\u003cstring, RateLimitEntry\u003e();\n// Keys: `${identifier}:${Math.floor(now / windowMs)}`\n// Old windows accumulate indefinitely\n```\n\n## Impact\n- **Severity**: Low-Medium (initially marked Critical, revised after analysis)\n- **Practical Risk**: Low for authenticated API with limited clients\n- **Memory Growth**: ~100 bytes per unique client per window\n\n## Root Cause\nNo cleanup mechanism exists for expired rate limit windows.\n\n## Solution\nAdd periodic cleanup that runs every minute to remove entries where `resetTime \u003c now`:\n\n```typescript\n// At module level, after store declaration\nsetInterval(() =\u003e {\n  const now = Date.now();\n  for (const [key, entry] of store) {\n    if (entry.resetTime \u003c now) {\n      store.delete(key);\n    }\n  }\n}, 60000);\n```\n\n## Testing\n1. Unit test: Verify entries are cleaned up after window expires\n2. Manual: Monitor memory under load\n\n## Files to Modify\n- `src/middleware/rate-limit.ts`\n\n## Acceptance Criteria\n- [ ] Cleanup interval added\n- [ ] Old entries removed after window expires\n- [ ] No performance regression\n- [ ] Existing tests pass","status":"closed","priority":2,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:02.545137513+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.838592718+01:00","closed_at":"2026-02-05T11:37:26.838592718+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.1","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:02.54840518+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.2","title":"Remove dead code: unused getFileMutex function","description":"## Problem\nThe `getFileMutex` function in `src/utils/mutex.ts` (lines 35-44) is exported but never called anywhere in the codebase. The original audit flagged this as a 'Critical memory leak' but upon investigation, the function is simply dead code.\n\n## Current State\n```typescript\nconst mutexes = new Map\u003cstring, Mutex\u003e();\n\nexport function getFileMutex(path: string): Mutex {\n  let mutex = mutexes.get(path);\n  if (!mutex) {\n    mutex = new Mutex();\n    mutexes.set(path, mutex);\n  }\n  return mutex;\n}\n```\n\n## Evidence of Non-Use\n- `FsKnownSitesAdapter`: Uses `private mutex = new Mutex()` (instance field)\n- `stats-storage.ts`: Uses `const statsMutex = new Mutex()` (module-level)\n- No imports of `getFileMutex` found in codebase\n\n## Impact\n- **Severity**: Low (dead code, not a runtime issue)\n- **Risk**: None (code is never executed)\n- **Benefit**: Cleaner codebase, reduces confusion\n\n## Solution\nRemove the `getFileMutex` function and its supporting `mutexes` Map.\n\n## Files to Modify\n- `src/utils/mutex.ts`: Remove lines 35-44\n\n## Acceptance Criteria\n- [ ] `getFileMutex` function removed\n- [ ] `mutexes` Map removed\n- [ ] TypeScript compiles without errors\n- [ ] No imports break (there are none)","status":"closed","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:13.507938338+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.857347685+01:00","closed_at":"2026-02-05T11:37:26.857347685+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.2","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:13.510346325+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.3","title":"Fix potential browser resource leak in loadPage exception path","description":"## Problem\nIn `src/adapters/puppeteer-browser.ts`, the `loadPage` method can leak browser instances if an exception occurs after `puppeteer.launch()` succeeds but before the pageId is returned and stored in `sessions`.\n\n## Current Flow\n```typescript\nasync loadPage(url: string, options?: LoadPageOptions): Promise\u003c{ pageId: string }\u003e {\n  const browser = await puppeteer.launch({ ... });  // 1. Browser created\n  const page = await browser.newPage();              // 2. Can throw here\n  // ... setup operations ...                         // 3. Can throw here\n  const pageId = `page-${++this.pageCounter}`;\n  this.sessions.set(pageId, { page, browser, userDataDir });  // 4. Only here is it tracked\n  return { pageId };\n}\n```\n\n## Failure Scenarios\n1. `browser.newPage()` throws → browser leaked\n2. `page.setUserAgent()` throws → browser + page leaked\n3. `page.goto()` throws → browser + page leaked\n4. Any operation before `sessions.set()` throws → leaked\n\n## Impact\n- **Severity**: Medium\n- **Likelihood**: Low (these operations rarely fail)\n- **Consequence**: Orphaned Chromium processes consuming memory/CPU\n- **Detection**: Hard to detect without process monitoring\n\n## Solution\nWrap the browser creation in try-catch and ensure cleanup:\n\n```typescript\nasync loadPage(url: string, options?: LoadPageOptions): Promise\u003c{ pageId: string }\u003e {\n  const userDataDir = await fs.promises.mkdtemp(...);\n  let browser: Browser | null = null;\n  \n  try {\n    browser = await puppeteer.launch({ ... });\n    const page = await browser.newPage();\n    // ... rest of setup ...\n    \n    const pageId = `page-${++this.pageCounter}`;\n    this.sessions.set(pageId, { page, browser, userDataDir });\n    return { pageId };\n  } catch (error) {\n    // Cleanup on failure\n    if (browser) {\n      await browser.close().catch(() =\u003e {});\n    }\n    await fs.promises.rm(userDataDir, { recursive: true, force: true }).catch(() =\u003e {});\n    throw error;\n  }\n}\n```\n\n## Additional Fix\nAlso convert `fs.mkdtempSync` to `fs.promises.mkdtemp` (async) while we're here.\n\n## Files to Modify\n- `src/adapters/puppeteer-browser.ts`: Lines 45-122\n\n## Testing\n1. Unit test: Mock puppeteer.launch to succeed, newPage to throw\n2. Verify browser.close() is called\n3. Verify userDataDir is cleaned up\n\n## Acceptance Criteria\n- [ ] Browser cleanup on exception\n- [ ] userDataDir cleanup on exception\n- [ ] Sync mkdtempSync converted to async\n- [ ] Existing tests pass\n- [ ] Add test for cleanup path","status":"closed","priority":2,"issue_type":"bug","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:32.310141086+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.874127127+01:00","closed_at":"2026-02-05T11:37:26.874127127+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.3","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:32.313857147+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.4","title":"Add error event handler to logger file stream","description":"## Problem\nThe logger in `src/utils/logger.ts` silently fails when file operations encounter errors. If the disk fills up or permissions change, log entries are lost without any notification.\n\n## Current Behavior\n```typescript\n// Line 50-52: Silent catch on init\n} catch (error) {\n  // Silently fail if config not ready yet - will retry on first log\n}\n\n// Line 66-70: Silent catch on write\ntry {\n  logFileStream.write(JSON.stringify(entry) + '\\n');\n} catch (error) {\n  // Fail silently to not disrupt the application\n}\n```\n\n## Impact\n- **Severity**: Low\n- **Consequence**: Disk issues go unnoticed; console output still works\n- **Detection**: Only noticed when reviewing logs and finding gaps\n\n## Solution\nAdd error event handler to the WriteStream and log failures to console:\n\n```typescript\nfunction initLogFile() {\n  if (logFileStream) return;\n  \n  try {\n    const dataDir = getDataDir();\n    const logDir = path.join(dataDir, 'logs');\n    fs.mkdirSync(logDir, { recursive: true });\n    \n    const timestamp = new Date().toISOString().split('T')[0];\n    const logFile = path.join(logDir, `scraper-${timestamp}.jsonl`);\n    \n    logFileStream = fs.createWriteStream(logFile, { flags: 'a' });\n    \n    // Add error handler\n    logFileStream.on('error', (err) =\u003e {\n      console.error('[LOGGER] File stream error:', err.message);\n      logFileStream = null; // Disable file logging, continue with console\n    });\n    \n    // ... rest of function\n  } catch (error) {\n    console.error('[LOGGER] Failed to initialize log file:', \n      error instanceof Error ? error.message : String(error));\n    // Console logging still works\n  }\n}\n```\n\n## Files to Modify\n- `src/utils/logger.ts`: Lines 31-53, 60-72\n\n## Acceptance Criteria\n- [ ] Error event handler added to WriteStream\n- [ ] Init errors logged to console (not silent)\n- [ ] Write errors logged to console (not silent)\n- [ ] File logging gracefully disabled on error\n- [ ] Console logging continues working","status":"closed","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:26:51.609594326+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.890205323+01:00","closed_at":"2026-02-05T11:37:26.890205323+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.4","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:26:51.614686256+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.5","title":"URL-encode credentials in buildSessionProxyUrl","description":"## Problem\nThe `buildSessionProxyUrl` function in `src/utils/proxy.ts` constructs proxy URLs by string concatenation without URL-encoding the credentials. If the login or password contains special characters (`@`, `:`, `/`, `%`), the resulting URL will be malformed.\n\n## Current Code\n```typescript\n// Line 76\nconst proxyUrl = `http://${sessionLogin}:${password}@${host}`;\n```\n\n## Example Failure\nIf password is `p@ss:word`, the URL becomes:\n```\nhttp://user:p@ss:word@host:port\n         ^  ^ These break URL parsing\n```\n\n## Impact\n- **Severity**: Low\n- **Likelihood**: Low (credentials come from environment variables set by operator)\n- **Consequence**: Proxy authentication fails with special characters\n\n## Note on Input Source\nThe credentials come from `DATADOME_PROXY_*` environment variables, which are operator-controlled (not user input). However, proper encoding is still best practice.\n\n## Solution\nUse `encodeURIComponent` for credentials:\n\n```typescript\nexport function buildSessionProxyUrl(\n  host: string,\n  login: string,\n  password: string,\n  sessionMinutes: number = 2\n): string {\n  const sessionId = crypto.randomUUID().slice(0, 8);\n  const baseLogin = login.split('-session-')[0];\n  const sessionLogin = `${baseLogin}-session-${sessionId}-sessTime-${sessionMinutes}`;\n  \n  // URL-encode credentials to handle special characters\n  const encodedLogin = encodeURIComponent(sessionLogin);\n  const encodedPassword = encodeURIComponent(password);\n  \n  return `http://${encodedLogin}:${encodedPassword}@${host}`;\n}\n```\n\n## Files to Modify\n- `src/utils/proxy.ts`: Lines 60-88\n\n## Testing\n1. Unit test with password containing `@`, `:`, `/`, `%`\n2. Verify URL parses correctly with `new URL()`\n\n## Acceptance Criteria\n- [ ] Credentials URL-encoded\n- [ ] Test added for special characters\n- [ ] Existing proxy tests pass","status":"closed","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:09.723331859+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.906436204+01:00","closed_at":"2026-02-05T11:37:26.906436204+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.5","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:09.727924794+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.6","title":"Add queue size limit to CoreScraperEngine","description":"## Problem\nThe PQueue in `src/core/engine.ts` has no maximum size limit. While rate limiting (10 req/min) provides some protection, the queue can still grow if requests arrive faster than they're processed.\n\n## Current Code\n```typescript\n// Line 22\nprivate queue = new PQueue({ concurrency: 5 });\n```\n\n## Impact\n- **Severity**: Low (mitigated by rate limiting)\n- **Likelihood**: Low (requires sustained high traffic beyond rate limit)\n- **Consequence**: Memory growth, eventual OOM\n\n## Analysis\nWith rate limiting at 10 req/min and scrape time ~120s, theoretical queue depth:\n- 10 requests/min × 2 min max = 20 requests in flight\n- Each request is just a Promise reference (~100 bytes)\n- Practical risk is minimal\n\n## Solution\nAdd explicit queue size check with informative error:\n\n```typescript\nexport class CoreScraperEngine {\n  private queue = new PQueue({ concurrency: 5 });\n  private static readonly MAX_QUEUE_SIZE = 100;\n\n  async scrapeUrl(url: string, options?: ScrapeOptions): Promise\u003cScrapeResult\u003e {\n    // Reject if queue is too deep (backpressure)\n    if (this.queue.size \u003e= CoreScraperEngine.MAX_QUEUE_SIZE) {\n      logger.warn('[ENGINE] Queue full, rejecting request', { \n        queueSize: this.queue.size, \n        url \n      });\n      return {\n        success: false,\n        errorType: ERROR_TYPES.CONFIGURATION,\n        error: 'Server overloaded, please retry later'\n      };\n    }\n\n    return await this.queue.add(async () =\u003e {\n      // ... existing code\n    });\n  }\n}\n```\n\n## Configuration\nConsider making MAX_QUEUE_SIZE configurable via environment variable in future.\n\n## Files to Modify\n- `src/core/engine.ts`: Lines 22, 48-74\n\n## Acceptance Criteria\n- [ ] MAX_QUEUE_SIZE constant added\n- [ ] Queue size check before adding\n- [ ] Informative error returned when queue full\n- [ ] Warning logged when queue full\n- [ ] Existing tests pass","status":"closed","priority":3,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:25.229337791+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.92260638+01:00","closed_at":"2026-02-05T11:37:26.92260638+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.6","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:25.232323063+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.7","title":"Validate LLM response Content-Type before parsing","description":"## Problem\nThe `OpenRouterLlmAdapter` in `src/adapters/openrouter-llm.ts` doesn't validate the Content-Type of the API response before accessing JSON fields. If OpenRouter returns an error page (HTML) with status 200, the error handling will be confusing.\n\n## Current Code\n```typescript\n// Lines 62-63\nconst content = response.data?.choices?.[0]?.message?.content;\nif (!content) return [];\n```\n\n## Impact\n- **Severity**: Low\n- **Likelihood**: Very low (APIs don't typically return HTML with 200)\n- **Consequence**: Confusing empty result instead of clear error\n\n## Note\nThe current code already handles this gracefully - if `choices` is undefined (as it would be for HTML), it returns `[]`. The fix is primarily for better error messages.\n\n## Solution\nAdd Content-Type validation and better logging:\n\n```typescript\ntry {\n  const response = await axios.post(...);\n  \n  // Validate response type\n  const contentType = response.headers['content-type'] || '';\n  if (!contentType.includes('application/json')) {\n    logger.warn('[LLM] Unexpected response type', { \n      contentType,\n      status: response.status \n    });\n    return [];\n  }\n  \n  const content = response.data?.choices?.[0]?.message?.content;\n  if (!content) {\n    logger.debug('[LLM] Empty content in response', { \n      hasChoices: !!response.data?.choices \n    });\n    return [];\n  }\n  \n  // ... rest of parsing\n} catch (error) {\n  // ... existing error handling\n}\n```\n\n## Files to Modify\n- `src/adapters/openrouter-llm.ts`: Lines 40-78\n\n## Acceptance Criteria\n- [ ] Content-Type validated\n- [ ] Warning logged for unexpected types\n- [ ] Debug logging for empty responses\n- [ ] Existing behavior preserved (return [] on failure)","status":"closed","priority":4,"issue_type":"task","owner":"bogorad@gmail","created_at":"2026-02-05T11:27:42.514385538+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.939327165+01:00","closed_at":"2026-02-05T11:37:26.939327165+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.7","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:27:42.518831805+01:00","created_by":"Eugene Bogorad"}]}
{"id":"smartScraper-y7m.9","title":"Update AUDIT_RESULT.md with critical analysis findings","description":"## Task\nUpdate the AUDIT_RESULT.md file with the critical analysis that distinguished valid issues from false positives.\n\n## Required Updates\n1. Add 'Critical Analysis' section showing validity assessment for each finding\n2. Mark issues that were false positives with clear explanation\n3. Mark issues that are 'by design' per ADRs with references\n4. Update severity levels based on analysis\n5. Add summary table of valid vs invalid findings\n\n## Key Findings from Analysis\n| # | Issue | Original | Revised | Reason |\n|---|-------|----------|---------|--------|\n| 2 | Mutex registry leak | Critical | N/A | Dead code - getFileMutex never called |\n| 3 | Process exit | High | By design | Documented in ADR-011 |\n| 6 | Stats race condition | Medium | N/A | Mutex already protects correctly |\n| 7 | XPath validation bypass | Medium | N/A | Browser sandbox prevents exploitation |\n| 11 | Session cookie httpOnly | Medium | N/A | Misread - httpOnly is always true |\n\n## Valid Issues (to be fixed)\n- #1 Rate limit store (Low-Medium)\n- #5 Sync file operations (Low) - bundled with #13\n- #8 CAPTCHA abort (Low)\n- #9 Logger error handling (Low)\n- #10 Proxy URL encoding (Low)\n- #12 Queue size limit (Low)\n- #13 Browser resource leak (Medium)\n- #14 LLM Content-Type (Low)\n\n## Files to Modify\n- AUDIT_RESULT.md\n\n## Acceptance Criteria\n- [ ] Each of 15 findings has validity assessment\n- [ ] False positives have clear explanations\n- [ ] ADR references for 'by design' items\n- [ ] Summary table shows valid count vs total","status":"closed","priority":3,"issue_type":"chore","owner":"bogorad@gmail","created_at":"2026-02-05T11:28:27.815542037+01:00","created_by":"Eugene Bogorad","updated_at":"2026-02-05T11:37:26.956719611+01:00","closed_at":"2026-02-05T11:37:26.956719611+01:00","close_reason":"Completed: All fixes implemented and verified","dependencies":[{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m","type":"parent-child","created_at":"2026-02-05T11:28:27.81878659+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.1","type":"blocks","created_at":"2026-02-05T11:28:36.530871516+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.2","type":"blocks","created_at":"2026-02-05T11:28:36.603524621+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.3","type":"blocks","created_at":"2026-02-05T11:28:36.676114718+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.4","type":"blocks","created_at":"2026-02-05T11:28:36.750211864+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.5","type":"blocks","created_at":"2026-02-05T11:28:36.8223172+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.6","type":"blocks","created_at":"2026-02-05T11:28:36.896562827+01:00","created_by":"Eugene Bogorad"},{"issue_id":"smartScraper-y7m.9","depends_on_id":"smartScraper-y7m.7","type":"blocks","created_at":"2026-02-05T11:28:36.96678639+01:00","created_by":"Eugene Bogorad"}]}
