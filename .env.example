# SmartScraper Configuration & Secrets
# Copy this file to .env and fill in your actual values
# All values are optional unless noted with [REQUIRED]

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Port on which the server will listen (default: 5555)
PORT=5555

# Node environment (default: production)
# Options: development, production
NODE_ENV=production

# ============================================================================
# DATA STORAGE
# ============================================================================

# Directory where logs, stats, and site configs are stored
# Creates data/logs, data/stats.json, data/sites.jsonc (default: ./data)
DATA_DIR=./data

# ============================================================================
# LLM CONFIGURATION (OpenRouter) [REQUIRED for XPath rediscovery]
# ============================================================================

# Your OpenRouter API Key
# Get one at: https://openrouter.ai
# Required for the LLM-assisted XPath discovery feature
OPENROUTER_API_KEY=your_openrouter_api_key_here

# The LLM model identifier to use
# Recommended: meta-llama/llama-4-maverick:free (large context window, free tier)
# Other options: gpt-4-turbo, claude-3-sonnet, etc.
# (default: meta-llama/llama-4-maverick:free)
LLM_MODEL=meta-llama/llama-4-maverick:free

# Temperature for LLM responses (0 = deterministic, higher = more creative)
# Range: 0-2 (default: 0)
LLM_TEMPERATURE=0

# HTTP Referer header for OpenRouter API requests
# (default: https://github.com/bogorad/smartScraper)
LLM_HTTP_REFERER=https://github.com/bogorad/smartScraper

# X-Title header for OpenRouter API requests
# (default: SmartScraper)
LLM_X_TITLE=SmartScraper

# ============================================================================
# BROWSER CONFIGURATION (Puppeteer)
# ============================================================================

# Path to your Chrome/Chromium executable
# Common paths:
#   - Linux: /usr/lib/chromium/chromium, /usr/bin/chromium, /snap/bin/chromium
#   - macOS: /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
#   - Windows: C:\Program Files\Google\Chrome\Application\chrome.exe
# (default: /usr/lib/chromium/chromium)
EXECUTABLE_PATH=/usr/lib/chromium/chromium

# Browser extensions to load (comma-separated paths)
# Example: /path/to/extension1,/path/to/extension2
# Useful for: ad-blocking, paywall bypass, fingerprint spoofing
EXTENSION_PATHS=

# HTTP proxy for web scraping (optional)
# Format: http://username:password@hostname:port
# Example: http://proxy-user:proxy-pass@proxy.example.com:8080
# Note: Also supports legacy name HTTP_PROXY
PROXY_SERVER=

# ============================================================================
# CAPTCHA SOLVER CONFIGURATION [REQUIRED if scraping sites with CAPTCHAs]
# ============================================================================

# Your 2Captcha API key
# Get one at: https://2captcha.com
# Required if your target sites use CAPTCHA protection
TWOCAPTCHA_API_KEY=your_2captcha_api_key_here

# Default timeout for CAPTCHA solving in seconds
# (default: 120)
CAPTCHA_DEFAULT_TIMEOUT=120

# Polling interval for CAPTCHA results in milliseconds
# (default: 5000)
CAPTCHA_POLLING_INTERVAL=5000

# ============================================================================
# DATADOME PROXY (For sites with DataDome protection)
# ============================================================================

# Residential proxy host and port for bypassing DataDome blocks
# Required when any site has needsProxy='datadome'
# Example: 170.106.118.114:2334
DATADOME_PROXY_HOST=

# Base proxy login/username (WITHOUT session parameters)
# Example: u76614f10561905ca-zone-custom-region-us
# Note: Do NOT include -session-xxx or -sessTime-xxx, the engine adds these
DATADOME_PROXY_LOGIN=

# Proxy password
# Example: u76614f10561905ca
DATADOME_PROXY_PASSWORD=

# ============================================================================
# AUTHENTICATION [REQUIRED for API access]
# ============================================================================

# API token for authenticating requests to /api/scrape endpoint
# Generate a secure random string: openssl rand -hex 32
# Can also be loaded from secrets.yaml (api_keys.smart_scraper)
API_TOKEN=your_secure_api_token_here

# ============================================================================
# LOGGING & DEBUG
# ============================================================================

# Log level for the application
# Options: DEBUG, INFO, WARN, ERROR, NONE
# DEBUG enables: verbose logs, HTML dumps, detailed error messages
# (default: INFO)
LOG_LEVEL=INFO

# Whether to save HTML snapshots on successful navigation
# Only active when LOG_LEVEL=DEBUG
# (default: false)
SAVE_HTML_ON_SUCCESS_NAV=false

# ============================================================================
# DOM STRUCTURE EXTRACTION (Advanced)
# ============================================================================

# Maximum text length to include in DOM structure dumps
# Lower values = smaller/faster, higher values = more context for LLM
# (default: 15)
DOM_STRUCTURE_MAX_TEXT_LENGTH=15

# Minimum text size to include in DOM structure annotations
# Helps filter out noise in DOM analysis
# (default: 100)
DOM_STRUCTURE_MIN_TEXT_SIZE_TO_ANNOTATE=100

# ============================================================================
# OPTIONAL: For development debugging
# ============================================================================

# Uncomment to enable detailed Node.js error messages
# NODE_ENV=development
